{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b6e27-d4c3-4fc5-9250-3ea2d6a2b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycococreatortools import pycococreatortools\n",
    "from icevision import models, parsers, show_records, tfms, Dataset, Metric, COCOMetric, COCOMetricType\n",
    "from icevision.imports import *\n",
    "from icevision.utils import *\n",
    "from icevision.data import *\n",
    "from icevision.metrics.metric import *\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9c97a-cc91-4d46-b254-31eb7f4cda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = models.torchvision.mask_rcnn\n",
    "backbone = model_type.backbones.resnet34_fpn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb615a8b-1fbb-4903-9f2b-6665016ac02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/root/\"\n",
    "mount_path = \"/root/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655cc998-28c1-44f8-90e8-1e9a619c2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = parsers.COCOMaskParser(annotations_filepath=f\"{data_path}/tile-cerulean-v2-partial-with-context/instances_Tiled Cerulean Dataset V2.json\", img_dir=f\"{mount_path}/tile-cerulean-v2-partial-with-context/tiled_images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092d049-96f2-4287-a1de-02b5033b2fe6",
   "metadata": {},
   "source": [
    "### Important! \n",
    "\n",
    "Make sure you have copied the dataset to the local SSD of the VM at /root. Loading the data from a GCP bucket takes a full 2 minutes compared to 17 seconds when data is on the SSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c906c-8fc2-4e14-b084-b040579eafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records, valid_records = parser.parse(autofix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbe064-698b-415b-9526-65b28abeea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    \"Infrastructure\": 1,\n",
    "    \"Natural Seep\": 2,\n",
    "    \"Coincident Vessel\": 3,\n",
    "    \"Recent Vessel\": 4,\n",
    "    \"Old Vessel\": 5,\n",
    "    \"Ambiguous\": 6,\n",
    "    \"Hard Negatives\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea5b87-476b-491d-bd17-50a02337f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=show_records(train_records[0:15], ncols=3, class_map=class_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97cdfc-1647-4f60-8c2c-bc1dcc4509fd",
   "metadata": {},
   "source": [
    "Normalizing is best practice and necessary for icevision to properly display predicition results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bbf0f-f603-4ae1-8e7c-d0620204fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = tfms.A.Adapter(\n",
    "    [\n",
    "        tfms.A.Normalize(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73d6aa-b3cf-4bb7-bfad-f13b9864d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size=512), tfms.A.Normalize()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fea564-f22a-4fda-b003-135bf0f2a89c",
   "metadata": {},
   "source": [
    "sourced from: https://airctic.com/0.8.1/getting_started_instance_segmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131dd0bf-ec6f-4d9e-95f6-2789ab3c3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"IoUMetric\", \"IoUMetricType\"]\n",
    "\n",
    "\n",
    "class IoUMetricType(Enum):\n",
    "    \"\"\"Available options for `COCOMetric`.\"\"\"\n",
    "\n",
    "    bbox = \"bbox\"\n",
    "    mask = \"segm\"\n",
    "    keypoint = \"keypoints\"\n",
    "\n",
    "\n",
    "class IoUMetric(Metric):\n",
    "    \"\"\"Wrapper around [cocoapi evaluator](https://github.com/cocodataset/cocoapi)\n",
    "    Calculates average precision.\n",
    "    # Arguments\n",
    "        metric_type: Dependent on the task you're solving.\n",
    "        print_summary: If `True`, prints a table with statistics.\n",
    "        show_pbar: If `True` shows pbar when preparing the data for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metric_type: IoUMetricType = IoUMetricType.mask,\n",
    "        iou_thresholds: Optional[Sequence[float]] = None,\n",
    "        print_summary: bool = False, \n",
    "        show_pbar: bool = True,\n",
    "    ):\n",
    "        self.metric_type = metric_type\n",
    "        self.iou_thresholds = iou_thresholds\n",
    "        self.print_summary = print_summary\n",
    "        self.show_pbar = show_pbar\n",
    "        self._records, self._preds = [], []\n",
    "\n",
    "    def _reset(self):\n",
    "        self._records.clear()\n",
    "        self._preds.clear()\n",
    "\n",
    "    def accumulate(self, preds):\n",
    "        for pred in preds:\n",
    "            self._records.append(pred.ground_truth)\n",
    "            self._preds.append(pred.pred)\n",
    "\n",
    "    def finalize(self) -> Dict[str, float]:\n",
    "        with CaptureStdout():\n",
    "            coco_eval = create_coco_eval(\n",
    "                records=self._records,\n",
    "                preds=self._preds,\n",
    "                metric_type=self.metric_type.value,\n",
    "                iou_thresholds=self.iou_thresholds,\n",
    "                show_pbar=self.show_pbar,\n",
    "            )\n",
    "            coco_eval.evaluate()\n",
    "            coco_eval.accumulate()\n",
    "\n",
    "        with CaptureStdout(propagate_stdout=self.print_summary):\n",
    "            coco_eval.summarize()\n",
    "\n",
    "        stats = coco_eval.stats\n",
    "        ious = coco_eval.ious\n",
    "\n",
    "        ious_l = []\n",
    "        for iou in ious.values():\n",
    "            if isinstance(iou, np.ndarray):\n",
    "                iou = iou.tolist()\n",
    "            else:\n",
    "                iou = iou\n",
    "            ious_l.append(iou)\n",
    "        \n",
    "        flat_ious_l = [item for sublist in ious_l for item in sublist]\n",
    "        if len(flat_ious_l) == 0:\n",
    "            flat_ious_l.append([0])\n",
    "        flat_ious_l = [item for items in flat_ious_l for item in items]\n",
    "        ious_avg = np.array(flat_ious_l).mean()\n",
    "        ious_min = np.array(flat_ious_l).min()\n",
    "        ious_max = np.array(flat_ious_l).max()\n",
    "        \n",
    "        logs = {\n",
    "            #\"Min IoU area=all\": ious_min,\n",
    "            #\"Max IoU area=all\": ious_max,\n",
    "            \"Avg. IoU area=all\": ious_avg, \n",
    "        }\n",
    "        self._reset()\n",
    "        \n",
    "        \n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92349ee5-2dd6-40e7-85f5-18f388958cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(train_records, train_tfms)\n",
    "valid_ds = Dataset(valid_records, valid_tfms)\n",
    "\n",
    "train_dl = model_type.train_dl(train_ds, batch_size=8, num_workers=6, shuffle=True) # adjust num_workers for your processor count\n",
    "valid_dl = model_type.valid_dl(valid_ds, batch_size=8, num_workers=6, shuffle=False)\n",
    "\n",
    "infer_dl = model_type.infer_dl(valid_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "model = model_type.model(backbone=backbone(pretrained=False), num_classes=len(parser.class_map))\n",
    "\n",
    "metrics = [IoUMetric(metric_type=IoUMetricType.mask)]\n",
    "\n",
    "learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aae8b0-8b1a-4ecd-a2b2-8b00562a1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c514a9-17d1-4ae1-be07-c72bab38e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.valley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27791bf2-ae0a-4592-af2a-99a9185f77e6",
   "metadata": {},
   "source": [
    "LR finder takes about a minute when data is on SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14b9ff-8233-4678-a4e2-407d49347455",
   "metadata": {},
   "source": [
    "1 train epoch is about 4 minutes. 1 validation epoch of 76 samples is also about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352a54b-26c8-48bf-91cc-97696cf45f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#learn.fine_tune(10, 3e-3) # 3e-3 is hand selected lr\n",
    "learn.fine_tune(10, lr.valley) #, freeze_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc1747-fb70-49a4-bac1-142c01c74e7e",
   "metadata": {},
   "source": [
    "a TODO is to debug the COCOMetric, it should not be -1 given that we are now acheiving detections that intersect with groundtruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a177c9-b2a6-4f50-a9a6-9f4458661d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {mount_path}/experiments/cv2/05062022_ep10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042c741-c3c7-4fa3-915e-a84ebe64d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'{mount_path}/experiments/cv2/05062022_ep10/05062022_ep10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c23c8-762f-4c2e-890b-ca92ee85fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"approximate time to train 30 epochs in minutes: {25*30/60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b181c1-7747-4b44-b8ea-ad1cc82f5b37",
   "metadata": {},
   "source": [
    "The predictions above .7 confidence that roughly line up with groundtruth demonstrates that icevision-trained models can produce predictions that look like they are headed in the correct direction, even for an imperfect training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529311f-7a75-4992-a92b-156e44e87832",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type.show_results??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1068f6-ea8a-42d7-a198-8b7a54a9d09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40d656-f929-4c1a-84ce-82a4fd29852c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = model_type.show_results(model, valid_ds, detection_threshold=.6) \n",
    "plt.savefig(\"inference_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2d097-ec7b-44c6-8a62-774ddda1cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_type.predict_from_dl(model, infer_dl, keep_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bd6d5-2354-4374-b2cf-59299eb8ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = preds[10].as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2645bd4-1958-4074-8b4e-6bb1c3b268d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8c5e8-110f-4e2d-ae4a-d9ef643e8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = valid_records[10].as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50b608-5703-47da-b5fb-3314055085df",
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1174e3-8fea-46f6-aaf4-6163117f4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_masks = v['detection']['masks'] #[0].to_mask(d['common']['height'],d['common']['width']).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24fcbe-54a7-4cdd-b25d-38b3f7bc5ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_masks = d['detection']['masks'] #[0].to_mask(d['common']['height'],d['common']['width']).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01980b87-b6c5-43a7-bf6e-f5e88ce01646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten our mask arrays and use scikit-learn to create a confusion matrix\n",
    "flat_preds = np.concatenate(d_masks).flatten()\n",
    "flat_truth = np.concatenate(v_masks).flatten()\n",
    "OUTPUT_CHANNELS = 6\n",
    "cm = confusion_matrix(flat_truth, flat_preds, labels=list(range(OUTPUT_CHANNELS)))\n",
    "\n",
    "classes = [1,2,3,4,5,6]\n",
    "\n",
    "#%matplotlib inline\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "# We want to show all ticks...\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       # ... and label them with the respective list entries\n",
    "       xticklabels=list(range(OUTPUT_CHANNELS)), yticklabels=list(range(OUTPUT_CHANNELS)),\n",
    "       title='Normalized Confusion Matrix',\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "fmt = '.2f' #'d' # if normalize else 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "fig.tight_layout(pad=2.0, h_pad=2.0, w_pad=2.0)\n",
    "ax.set_ylim(len(classes)-0.5, -0.5)\n",
    "\n",
    "plt.savefig(f'{ROOT_DIR}cm.png')\n",
    "\n",
    "\n",
    "# compute f1 score\n",
    "f1 = f1_score(flat_truth, flat_preds, average='macro')\n",
    "\n",
    "print(\"cm: \", cm)\n",
    "print(\"f1: \", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai2]",
   "language": "python",
   "name": "conda-env-fastai2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
