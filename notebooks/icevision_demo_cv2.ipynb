{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8985ab-8b93-473f-8d94-af4d2fec7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b6e27-d4c3-4fc5-9250-3ea2d6a2b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycococreatortools import pycococreatortools\n",
    "from icevision import models, parsers, show_records, tfms, Dataset, Metric, COCOMetric, COCOMetricType\n",
    "from icevision.imports import *\n",
    "from icevision.utils import *\n",
    "from icevision.data import *\n",
    "from icevision.metrics.metric import *\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import skimage.io as skio\n",
    "from ceruleanml.coco_stats import all_sample_stat_lists\n",
    "from ceruleanml.coco_load_fastai import record_collection_to_record_ids, get_image_path, record_to_mask\n",
    "from ceruleanml import preprocess\n",
    "from ceruleanml import data\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "from icevision.metrics import SimpleConfusionMatrix # make sure you have the rbavery fork of icevision installed\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebc5fe-de42-43ef-a795-d4dad9bbb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_d ={512:14, 256:28, 224:28, 128:50, 64:128} # Batch Size for each image size\n",
    "lr_d = {512:3e-4, 256:1e-3, 224:3e-3, 128:3e-3, 64:1e-2} # Learning Rate for each image size\n",
    "# the timeings below are safe on a T4. spikes in gpu mem for smaller im sizes \n",
    "# with large batch size cause OOM. 34 bs for 224 for example\n",
    "mins_d = {512:5.01, 256:3.1, 224:3.1, 128:2.5, 64:1.6} \n",
    "run_list = [[512, 5*60]]*1 # List of tuples, where the tuples are [px size, training time in minutes]\n",
    "# run_list = [[64, 1]]*1+[[128, 1]]*1+[[224, 1]]*1 +[[512, 1]]*1\n",
    "init_size = run_list[0][0]\n",
    "fp16 = False\n",
    "n = \"all\"\n",
    "arch = 34\n",
    "negative_sample_count = 500\n",
    "negative_sample_count_val = 50\n",
    "area_thresh = 0\n",
    "classes_to_remove=[\n",
    "    \"ambiguous\",\n",
    "    ]\n",
    "classes_to_remap ={\n",
    "    # \"old_vessel\": \"recent_vessel\",\n",
    "    # \"coincident_vessel\": \"recent_vessel\",\n",
    "}\n",
    "\n",
    "model_type = models.torchvision.mask_rcnn\n",
    "backbone = model_type.backbones.resnet34_fpn\n",
    "# causes kernel death without any CUDA out of memory error or traceback aftr training for a while\n",
    "# model_type = models.mmdet.mask_rcnn\n",
    "# backbone = model_type.backbones.mask_rcnn_swin_t_p4_w7_fpn_1x_coco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783dd8a3-faaf-46bd-a017-884e2bb71ecb",
   "metadata": {},
   "source": [
    "### Important! \n",
    "\n",
    "Make sure you have copied the dataset to the local SSD of the VM at /root. Loading the data from a GCP bucket takes a full 2 minutes compared to 17 seconds when data is on the SSD.\n",
    "\n",
    "You can run the following for example to copy a dataset from the bucket to the vm quickly.\n",
    "\n",
    "```\n",
    "mkdir tile-cerulean-v2-partial-with-context\n",
    "gsutil -m rsync -ravzp gs://ceruleanml/tile-cerulean-v2-partial-with-context tile-cerulean-v2-partial-with-context\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_held_scenes(f_paths):\n",
    "    held_scenes = []\n",
    "    for f_path in f_paths:\n",
    "        with open(f_path) as f:\n",
    "            data = f.read().split(\"\\n\")\n",
    "            held_scenes += [line.split(\"/\")[-1] for line in data]\n",
    "    return held_scenes\n",
    "test_scenes = f\"/root/data/partitions/test_scenes.txt\"\n",
    "val_scenes = f\"/root/data/partitions/val_scenes.txt\"\n",
    "held_scenes = get_held_scenes([test_scenes, val_scenes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81217b0b-46b6-4979-b5d3-1926a194aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_path = \"/root\"\n",
    "\n",
    "### Parsing COCO Dataset with Icevision\n",
    "\n",
    "train_set = \"train-with-context-512\"\n",
    "tiled_images_folder_train = \"tiled_images\"\n",
    "json_name_train = \"instances_TiledCeruleanDatasetV2.json\"\n",
    "\n",
    "coco_json_path_train = f\"{mount_path}/partitions/{train_set}/{json_name_train}\"\n",
    "tiled_images_folder_train = f\"{mount_path}/partitions/{train_set}/{tiled_images_folder_train}\"\n",
    "val_set = \"val-with-context-512\"\n",
    "tiled_images_folder_val= \"tiled_images\"\n",
    "json_name_val = \"instances_TiledCeruleanDatasetV2.json\"\n",
    "coco_json_path_val= f\"{mount_path}/partitions/{val_set}/{json_name_val}\"\n",
    "tiled_images_folder_val = f\"{mount_path}/partitions/{val_set}/{tiled_images_folder_val}\"\n",
    "\n",
    "# f\"{mount_path}/partitions/val/instances_tiled_cerulean_train_v2.json\"\n",
    "\n",
    "## looking at area distribution to find area threshold\n",
    "\n",
    "# df = preprocess.get_area_df(coco_json_path_train, tiled_images_folder_train)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b54cc5-c6b1-46f8-98d1-ddbce69a382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_collection_with_negative_small_filtered_train = preprocess.load_set_record_collection(\n",
    "    coco_json_path_train, tiled_images_folder_train, area_thresh, negative_sample_count, preprocess=True,\n",
    "    classes_to_remap=classes_to_remap, classes_to_remove=classes_to_remove, held_scenes=held_scenes\n",
    ")\n",
    "record_ids_train = record_collection_to_record_ids(record_collection_with_negative_small_filtered_train)\n",
    "\n",
    "record_collection_with_negative_small_filtered_val = preprocess.load_set_record_collection(\n",
    "    coco_json_path_val, tiled_images_folder_val, area_thresh, negative_sample_count_val, preprocess=True,\n",
    "    classes_to_remap=classes_to_remap, classes_to_remove=classes_to_remove\n",
    ")\n",
    "record_ids_val = record_collection_to_record_ids(record_collection_with_negative_small_filtered_val)\n",
    "\n",
    "assert len(set(record_ids_train)) + len(set(record_ids_val)) == len(record_ids_train) + len(record_ids_val)\n",
    "\n",
    "train_val_record_ids = record_ids_train + record_ids_val\n",
    "combined_record_collection = record_collection_with_negative_small_filtered_train + record_collection_with_negative_small_filtered_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b9f03",
   "metadata": {},
   "source": [
    "necessary step is to clone rbavery icevision and isntall it into environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2671854-0646-42a0-b33d-92c8ddd28701",
   "metadata": {},
   "source": [
    "This func is adapted in the icevision fork to only show the first channel of the three channel dataset in `draw_sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea5b87-476b-491d-bd17-50a02337f05e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_records(record_collection_with_negative_small_filtered_train[0:3], ncols=3, class_map=data.class_list, display_mask=True, display_bbox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fea564-f22a-4fda-b003-135bf0f2a89c",
   "metadata": {},
   "source": [
    "sourced from: https://airctic.com/0.8.1/getting_started_instance_segmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a7dca-2fc2-466d-8c0b-41bc0962a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size=init_size)])\n",
    "train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(\n",
    "    size = init_size, \n",
    "    shift_scale_rotate = tfms.A.ShiftScaleRotate(\n",
    "        p=1,\n",
    "        scale_limit=0.01,\n",
    "        rotate_limit=180,\n",
    "        border_mode=0, # cv2.BORDER_CONSTANT\n",
    "        value=[124, 116, 104], # default gray\n",
    "        mask_value=0,\n",
    "    ),\n",
    "    lighting = tfms.A.RandomBrightnessContrast(p=1),\n",
    "    crop_fn = None,\n",
    "    blur = None\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92349ee5-2dd6-40e7-85f5-18f388958cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset(record_collection_with_negative_small_filtered_train, train_tfms)\n",
    "valid_ds = Dataset(record_collection_with_negative_small_filtered_val, valid_tfms)\n",
    "train_dl = model_type.train_dl(train_ds, batch_size=32, num_workers=6, shuffle=True) # adjust num_workers for your processor count\n",
    "valid_dl = model_type.valid_dl(valid_ds, batch_size=32, num_workers=6, shuffle=False)\n",
    "infer_dl = model_type.infer_dl(valid_ds, batch_size=32, shuffle=False)\n",
    "metrics = [SimpleConfusionMatrix(print_summary=True, class_list=data.class_list)]\n",
    "\n",
    "model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(data.class_list))\n",
    "learner = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, cbs=SaveModelCallback(min_delta=.01), metrics=metrics) # cbs=SaveModelCallback\n",
    "\n",
    "running_total_epochs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14b9ff-8233-4678-a4e2-407d49347455",
   "metadata": {},
   "source": [
    "1 train epoch is about 4 minutes. 1 validation epoch of 76 samples is also about a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde2937-c7e3-4c12-a1f5-3f4b034d6811",
   "metadata": {},
   "source": [
    "# No Progressive Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352a54b-26c8-48bf-91cc-97696cf45f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#learner.fine_tune(35, 3e-3) # 3e-3 is hand selected lr\n",
    "# learn.fine_tune(2, lr.valley) #, freeze_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c9e67-0aee-461d-9453-60507146040a",
   "metadata": {},
   "source": [
    "## Progressive Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a26368-f4e8-445d-ba0a-067c61539e69",
   "metadata": {},
   "source": [
    "We save the model first if using savemodel callback or else there is an error saying model.pth does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad5436-4dd4-4877-ac91-cb68dbf9ba2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from icevision.models.utils import get_dataloaders\n",
    "from icevision.engines.fastai import convert_dataloaders_to_fastai\n",
    "\n",
    "for size, total_train_time in run_list:\n",
    "    epochs = max(int(total_train_time/mins_d[size]), 1)\n",
    "    bs = bs_d[size]\n",
    "    lr = lr_d[size]\n",
    "    valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size=size)])\n",
    "    train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(\n",
    "        size = size, \n",
    "        shift_scale_rotate = tfms.A.ShiftScaleRotate(\n",
    "            p=1,\n",
    "            scale_limit=0.01,\n",
    "            rotate_limit=180,\n",
    "            border_mode=0, # cv2.BORDER_CONSTANT\n",
    "            value=[124, 116, 104], # default gray\n",
    "            mask_value=0,\n",
    "        ),\n",
    "        lighting = tfms.A.RandomBrightnessContrast(p=1),\n",
    "        crop_fn = None,\n",
    "        blur = None\n",
    "    )])\n",
    "    ds, dls = get_dataloaders(model_type, [record_collection_with_negative_small_filtered_train, record_collection_with_negative_small_filtered_val], [train_tfms, valid_tfms], batch_size=bs, num_workers=8)\n",
    "    fastai_dls = convert_dataloaders_to_fastai(dls=dls)\n",
    "    learner.dls = fastai_dls\n",
    "    print(f\"Training time is: {total_train_time} minutes\")\n",
    "    print(\"starting from running total\", running_total_epochs)\n",
    "    print(\"image size\", size)\n",
    "    print(\"batch size\", bs)\n",
    "    print(\"arch\", arch)\n",
    "    print(\"lr\", lr)\n",
    "    print(\"n chips\", n)\n",
    "    print(\"context\")\n",
    "    print(\"epochs\", epochs)\n",
    "    print(\"num_classes\", len(data.class_list))\n",
    "\n",
    "    learner.fine_tune(epochs, lr, freeze_epochs=0) # cbs=cbs\n",
    "\n",
    "    running_total_epochs[size] = sum(filter(None,[running_total_epochs.get(size),epochs]))\n",
    "\n",
    "learner.save(\"single_truth_5h\")\n",
    "from datetime import datetime\n",
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
    "experiment_dir =  Path(f'{mount_path}/experiments/cv2/'+timestampStr+'_icevision_maskrcnn/')\n",
    "experiment_dir.mkdir(exist_ok=True)\n",
    "print(experiment_dir)\n",
    "from ceruleanml.inference import save_icevision_model_state_dict_and_tracing, load_tracing_model, test_tracing_model_one_batch, logits_to_classes\n",
    "save_template = \"model.pt\"\n",
    "state_dict_pth, tracing_model_cpu_pth  = save_icevision_model_state_dict_and_tracing(learner, save_template, experiment_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985f0bf-cfe5-4575-adbc-70199a8518e1",
   "metadata": {},
   "source": [
    "# Instance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d3f988-18cd-434e-aa0a-ed948159d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"7hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load(\"60min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308c2ec-463e-48ba-ab6e-54f5a7cffa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = learner.load(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aeea3d-3003-4ba4-aacc-88f0661b0113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type.show_results(model, valid_ds, detection_threshold=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224d4df-60dd-472f-9758-8526ead52cc4",
   "metadata": {},
   "source": [
    "TODO savemodel callback bugs this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712d2a3-4df3-4c71-a6fd-5770a98693d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation = learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5529b-c896-499d-84f0-c2bd1a123601",
   "metadata": {},
   "source": [
    "# Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a177c9-b2a6-4f50-a9a6-9f4458661d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
    "experiment_dir =  Path(f'{mount_path}/experiments/cv2/'+timestampStr+'_icevision_maskrcnn/')\n",
    "experiment_dir.mkdir(exist_ok=True)\n",
    "print(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f97030-2c22-4e41-9e6f-396e1d2ad47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ceruleanml.inference import save_icevision_model_state_dict_and_tracing, load_tracing_model, test_tracing_model_one_batch, logits_to_classes\n",
    "save_template = \"model.pt\"\n",
    "state_dict_pth, tracing_model_cpu_pth  = save_icevision_model_state_dict_and_tracing(learner, save_template, experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166b76f-cb90-421a-ae03-1a6e5eabb5b5",
   "metadata": {},
   "source": [
    "# Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117cb99-022c-43bc-82da-981314b62fac",
   "metadata": {},
   "source": [
    "* `nvidia-smi -lms` reports at millisecond frequency and can reveal big gpu spikes\n",
    "* a TODO is to debug the COCOMetric, it should not be -1 given that we are now acheiving detections that intersect with groundtruth. It's documented in icevision issues that the COCOMetric doesn't work for torchvision models because of a bounding box coordinate conversion error. They say it works for mmdet but mmdet does not support negative samples and was erroring saying there were negative samples even when none were included and no data transformations were done.\n",
    "* for the icevision trainer, class mismatch from preprocess remap causes long pauses then training failure. restarting kernel causes . reboot causes driver removal and need to redeploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b026a4a-d8f2-46e6-bee1-9823b63d1b50",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* save model, run inference\n",
    "* remove classes from json and resave\n",
    "* dicemulti metric for icevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3f3c6-63ee-49f8-ad12-f5c016b18b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icevision",
   "language": "python",
   "name": "icevision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
