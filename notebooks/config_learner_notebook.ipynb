{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from icevision import models, tfms\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from ceruleanml import coco_load_fastai, data, preprocess\n",
        "from ceruleanml.coco_load_fastai import record_collection_to_record_ids, get_image_path, record_to_mask\n",
        "from torchsummary import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memtile_size = 1024  # setting memtile_size=0 means use full scenes instead of tiling\n",
        "rrctile_size = 1024  #\n",
        "run_list = [\n",
        "    [512, 80],\n",
        "    # [416, 60],\n",
        "]  # List of tuples, where the tuples are [px size, training time in minutes]\n",
        "\n",
        "negative_sample_count_train = 100\n",
        "negative_sample_count_val = 0\n",
        "negative_sample_count_test = 0\n",
        "negative_sample_count_rrctrained = 0\n",
        "\n",
        "area_thresh = 100  # XXX maybe run a histogram on this to confirm that we have much more than 100 px normally!\n",
        "\n",
        "classes_to_remove = [\n",
        "    \"ambiguous\",\n",
        "    # \"natural_seep\",\n",
        "]\n",
        "classes_to_remap = {\n",
        "    \"old_vessel\": \"recent_vessel\",\n",
        "    \"coincident_vessel\": \"recent_vessel\",\n",
        "}\n",
        "\n",
        "classes_to_keep = [\n",
        "    c\n",
        "    for c in data.class_list\n",
        "    if c not in classes_to_remove + list(classes_to_remap.keys())\n",
        "]\n",
        "\n",
        "thresholds = {\n",
        "    \"pixel_nms_thresh\": 0.4,  # prediction vs itself, pixels\n",
        "    \"bbox_score_thresh\": 0.2,  # prediction vs score, bbox\n",
        "    \"poly_score_thresh\": 0.2,  # prediction vs score, polygon\n",
        "    \"pixel_score_thresh\": 0.2,  # prediction vs score, pixels\n",
        "    \"groundtruth_dice_thresh\": 0.0,  # prediction vs ground truth, theshold\n",
        "}\n",
        "\n",
        "num_workers = 8  # based on processor, but I don't know how to calculate..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_type = models.torchvision.mask_rcnn\n",
        "backbone = model_type.backbones.resnext101_32x8d_fpn\n",
        "model = model_type.model(\n",
        "    backbone=backbone(pretrained=True),\n",
        "    num_classes=len(classes_to_keep),\n",
        "    box_nms_thresh=0.5,\n",
        "    mask_roi_pool=MultiScaleRoIAlign(\n",
        "        featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=14 * 4, sampling_ratio=2\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regularization\n",
        "wd = 0.01\n",
        "\n",
        "\n",
        "# Ablation studies for aux channels\n",
        "def triplicate(img, **params):\n",
        "    img[..., :] = img[..., 0:1]\n",
        "    return img\n",
        "\n",
        "\n",
        "def sat_mask(img, **params):\n",
        "    img[..., :] = img[..., 0:1]\n",
        "    img[..., 2] = img[..., 2] != 0\n",
        "    return img\n",
        "\n",
        "\n",
        "def vessel_traffic(img, **params):\n",
        "    img[..., 1] = img[..., 0]\n",
        "    return img\n",
        "\n",
        "\n",
        "def infra_distance(img, **params):\n",
        "    img[..., 2] = img[..., 0]\n",
        "    return img\n",
        "\n",
        "\n",
        "def no_op(img, **params):\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_tfms(\n",
        "    memtile_size=memtile_size,\n",
        "    rrctile_size=rrctile_size,\n",
        "    reduced_resolution_tile_size=run_list[-1][0],\n",
        "    scale_limit=0.05,\n",
        "    rotate_limit=10,\n",
        "    border_mode=0,  # cv2.BORDER_CONSTANT, use pad_fill_value\n",
        "    pad_fill_value=[0, 0, 0],  # no_value\n",
        "    mask_value=0,\n",
        "    interpolation=0,  # cv2.INTER_NEAREST\n",
        "    r_shift_limit=10,  # SAR Imagery\n",
        "    g_shift_limit=0,  # Infrastructure Vicinity\n",
        "    b_shift_limit=0,  # Vessel Density\n",
        "):\n",
        "    train_tfms = tfms.A.Adapter(\n",
        "        [\n",
        "            tfms.A.Flip(\n",
        "                p=0.5,\n",
        "            ),\n",
        "            tfms.A.Affine(\n",
        "                p=1,\n",
        "                scale=(1 - scale_limit, 1 + scale_limit),\n",
        "                rotate=[-rotate_limit, rotate_limit],\n",
        "                interpolation=interpolation,\n",
        "                mode=border_mode,\n",
        "                cval=pad_fill_value,\n",
        "                cval_mask=mask_value,\n",
        "                fit_output=True,\n",
        "            ),\n",
        "            tfms.A.RandomSizedCrop(\n",
        "                p=1,\n",
        "                min_max_height=[rrctile_size, rrctile_size],\n",
        "                height=reduced_resolution_tile_size,\n",
        "                width=reduced_resolution_tile_size,\n",
        "                w2h_ratio=1,\n",
        "                interpolation=interpolation,\n",
        "            ),\n",
        "            tfms.A.RGBShift(\n",
        "                p=1,\n",
        "                r_shift_limit=r_shift_limit,\n",
        "                g_shift_limit=g_shift_limit,\n",
        "                b_shift_limit=b_shift_limit,\n",
        "            ),\n",
        "            tfms.A.Lambda(p=1, image=no_op),\n",
        "        ]\n",
        "    )\n",
        "    valid_tfms = tfms.A.Adapter(\n",
        "        [\n",
        "            tfms.A.RandomSizedCrop(\n",
        "                p=1,\n",
        "                min_max_height=[rrctile_size, rrctile_size],\n",
        "                height=reduced_resolution_tile_size,\n",
        "                width=reduced_resolution_tile_size,\n",
        "                w2h_ratio=1,\n",
        "                interpolation=interpolation,\n",
        "            ),\n",
        "            tfms.A.Lambda(p=1, image=no_op),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return [train_tfms, valid_tfms]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datasets\n",
        "mount_path = \"/root\"\n",
        "\n",
        "# Parsing COCO Dataset with Icevision\n",
        "json_name = \"instances_TiledCeruleanDatasetV2.json\"\n",
        "\n",
        "train_set = f\"train_tiles_context_{memtile_size}\"\n",
        "coco_json_path_train = f\"{mount_path}/partitions/{train_set}/{json_name}\"\n",
        "tiled_images_folder_train = f\"{mount_path}/partitions/{train_set}/tiled_images\"\n",
        "\n",
        "val_set = f\"val_tiles_context_{rrctile_size}\"\n",
        "coco_json_path_val = f\"{mount_path}/partitions/{val_set}/{json_name}\"\n",
        "tiled_images_folder_val = f\"{mount_path}/partitions/{val_set}/tiled_images\"\n",
        "\n",
        "test_set = f\"test_tiles_context_{rrctile_size}\"\n",
        "coco_json_path_test = f\"{mount_path}/partitions/{test_set}/{json_name}\"\n",
        "tiled_images_folder_test = f\"{mount_path}/partitions/{test_set}/tiled_images\"\n",
        "\n",
        "rrctrained_set = f\"train_tiles_context_{rrctile_size}\"\n",
        "coco_json_path_rrctrained = f\"{mount_path}/partitions/{rrctrained_set}/{json_name}\"\n",
        "\n",
        "tiled_images_folder_rrctrained = (\n",
        "    f\"{mount_path}/partitions/{rrctrained_set}/tiled_images\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record_collection_train = preprocess.load_set_record_collection(\n",
        "    coco_json_path_train,\n",
        "    tiled_images_folder_train,\n",
        "    area_thresh,\n",
        "    negative_sample_count_train,\n",
        "    preprocess=True,\n",
        "    classes_to_remap=classes_to_remap,\n",
        "    classes_to_remove=classes_to_remove,\n",
        "    classes_to_keep=classes_to_keep,\n",
        ")\n",
        "\n",
        "record_collection_val = preprocess.load_set_record_collection(\n",
        "    coco_json_path_val,\n",
        "    tiled_images_folder_val,\n",
        "    area_thresh,\n",
        "    negative_sample_count_val,\n",
        "    preprocess=True,\n",
        "    classes_to_remap=classes_to_remap,\n",
        "    classes_to_remove=classes_to_remove,\n",
        "    classes_to_keep=classes_to_keep,\n",
        ")\n",
        "\n",
        "record_collection_test = preprocess.load_set_record_collection(\n",
        "    coco_json_path_test,\n",
        "    tiled_images_folder_test,\n",
        "    area_thresh,\n",
        "    negative_sample_count_test,\n",
        "    preprocess=True,\n",
        "    classes_to_remap=classes_to_remap,\n",
        "    classes_to_remove=classes_to_remove,\n",
        "    classes_to_keep=classes_to_keep,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record_ids_train = coco_load_fastai.record_collection_to_record_ids(\n",
        "    record_collection_train\n",
        ")\n",
        "record_ids_val = coco_load_fastai.record_collection_to_record_ids(record_collection_val)\n",
        "record_ids_test = coco_load_fastai.record_collection_to_record_ids(\n",
        "    record_collection_test\n",
        ")\n",
        "\n",
        "# Create name for model based on parameters above\n",
        "model_name = f\"{len(classes_to_keep)}cls_rnxt101_pr{run_list[-1][0]}_px{rrctile_size}_{sum([r[1] for r in run_list])}min\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(record_collection_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "from fastai.callback.fp16 import *\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "# from torchsummary import summary\n",
        "import json\n",
        "import wandb\n",
        "# from fastai.callback.wandb import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes_to_keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from ceruleanml import data\n",
        "# from ceruleanml import evaluation\n",
        "# from ceruleanml import preprocess\n",
        "# from fastai.data.block import DataBlock\n",
        "# from fastai.vision.data import ImageBlock, MaskBlock\n",
        "# from fastai.vision.augment import aug_transforms, Resize\n",
        "# from fastai.vision.learner import unet_learner\n",
        "# from fastai.data.transforms import IndexSplitter\n",
        "# from fastai.metrics import DiceMulti, Dice, accuracy_multi, PrecisionMulti, RecallMulti\n",
        "# from fastai.callback.fp16 import MixedPrecision\n",
        "# # from fastai.callback.tensorboard import TensorBoardCallback\n",
        "# from fastai.vision.core import PILImageBW\n",
        "# from datetime import datetime\n",
        "# from pathlib import Path\n",
        "# import os, random\n",
        "# from icevision.visualize import show_data\n",
        "# import torch\n",
        "# from fastai.callback.tracker import EarlyStoppingCallback, SaveModelCallback\n",
        "# import skimage.io as skio\n",
        "# import numpy as np\n",
        "# from math import log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_val_record_ids = record_ids_train + record_ids_val\n",
        "# combined_record_collection = record_collection_with_negative_small_filtered_train + record_collection_with_negative_small_filtered_val\n",
        "combined_record_collection = record_collection_train + record_collection_val\n",
        "def get_val_indices(combined_ids, val_ids):\n",
        "    return list(range(len(combined_ids)))[-len(val_ids):]\n",
        "\n",
        "#show_data.show_records(random.choices(combined_train_records, k=9), ncols=3)\n",
        "\n",
        "### Constructing a FastAI DataBlock that uses parsed COCO Dataset from icevision parser. aug_transforms can only be used with_context=True\n",
        "\n",
        "val_indices = get_val_indices(train_val_record_ids, record_ids_val)\n",
        "\n",
        "def get_image_by_record_id(record_id):\n",
        "    return get_image_path(combined_record_collection, record_id)\n",
        "\n",
        "def get_mask_by_record_id(record_id):\n",
        "    return record_to_mask(combined_record_collection, record_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Fastai DataLoaders for grayscale images\n",
        "path_seg = \"/root/work/masked_tiles\"  # Dataset's path\n",
        "ParentSplitter_seg = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
        "SAR_stats = [0.2087162, 0.13736105] # Calculated from the entire training dataset\n",
        "\n",
        "codes = ['background', 'infrastructure', 'natural', 'vessel_coincident', 'vessel_recent', 'vessel_old', 'ambiguous']\n",
        "\n",
        "# cbs_seg = [WandbCallback(log_model=True),TerminateOnNaNCallback(), GradientAccumulation(8), GradientClip(), SaveModelCallback(), ShowGraphCallback()]\n",
        "cbs_seg = [TerminateOnNaNCallback(), GradientAccumulation(8), GradientClip(), SaveModelCallback(), ShowGraphCallback()]\n",
        "\n",
        "#  ShortEpochCallback(pct=0.1, short_valid=False),\n",
        "# EarlyStoppingCallback(min_delta=.001, patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "size = 512\n",
        "bs = 16\n",
        "\n",
        "batch_transfms = [*aug_transforms(flip_vert=True, max_rotate=180, max_warp=0.1, size=size)]\n",
        "coco_seg_dblock = DataBlock(\n",
        "        blocks=(ImageBlock, MaskBlock(codes=data.class_list)), # ImageBlock is RGB by default, uses PIL\n",
        "        get_x=get_image_by_record_id,\n",
        "        splitter=IndexSplitter(val_indices),\n",
        "        get_y=get_mask_by_record_id,\n",
        "        batch_tfms=batch_transfms,\n",
        "        item_tfms = Resize(size),\n",
        "        n_inp=1\n",
        "    )\n",
        "\n",
        "\n",
        "dls = coco_seg_dblock.dataloaders(source=train_val_record_ids, batch_size=bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.class_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=convnext_small()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "body = create_body(model, 3, pretrained=True)\n",
        "unet = DynamicUnet(body[0], n_out=7, img_size = (128,128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(unet, (3,512,512))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_func = CrossEntropyLossFlat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unet_learn = Learner(dls, unet, loss_func=loss_func, cbs=cbs_seg, lr=1e-3, wd=wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dls.show_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unet_learn.show_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs, targets = unet_learn.dls.train.one_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
