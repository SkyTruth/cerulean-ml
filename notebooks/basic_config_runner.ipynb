{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bENUAXPgDEkl"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "from fastai.callback.fp16 import *\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "import json\n",
        "import wandb\n",
        "from fastai.callback.wandb import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learner_config = json.load(open(\"/root/work/starter_learner_config.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.init(project='cv3-experiments', config=learner_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model / Learner params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freeze_epochs =  learner_config[\"freeze_epochs\"] #\n",
        "unfreeze_epochs = learner_config[\"unfreeze_epochs\"] #\n",
        "loss_func_name = learner_config[\"loss_func\"] #\n",
        "w_d = learner_config[\"w_d\"] #\n",
        "backbone = learner_config[\"backbone\"] #\n",
        "final_resize_resolution = learner_config[\"final_resize_resolution\"] #\n",
        "num_of_workers = learner_config[\"num_of_workers\"] #\n",
        "progressive_resizing = learner_config[\"progressive_resizing\"] #\n",
        "batch_size = learner_config[\"batch_size\"] #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Params (Not Relevent to this script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = learner_config[\"classes\"]\n",
        "area_thresh = learner_config[\"area_thresh\"]\n",
        "negative_sample_count_train = learner_config[\"negative_sample_count_train\"]\n",
        "aux_channels = learner_config[\"aux_channels\"]\n",
        "scale_limit = learner_config[\"scale_limit\"]\n",
        "rotate_limit = learner_config[\"rotate_limit\"]\n",
        "r_g_b_shift_limit = learner_config[\"r/g/b_shift_limit\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVov7m_DP2k4"
      },
      "outputs": [],
      "source": [
        "class NormalizeAndClamp(Transform):\n",
        "    def __init__(self, mean, std, min_val=-3, max_val=3):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.min_val = min_val\n",
        "        self.max_val = max_val\n",
        "\n",
        "    def encodes(self, x):\n",
        "        x = (x - self.mean) / self.std\n",
        "        x = x.clamp(min=self.min_val, max=self.max_val)\n",
        "        return x\n",
        "\n",
        "def get_image_files_no_masks(path, pct_negative=0.0):\n",
        "    \"\"\"Collect image files that don't end with '_mask.png'.\"\"\"\n",
        "    empty_masks = []\n",
        "    non_empty_masks = []\n",
        "\n",
        "    all_files = get_image_files(path)\n",
        "    masks = [f for f in all_files if f.name.endswith(\"_mask.png\")]\n",
        "\n",
        "    for m in tqdm(masks):\n",
        "        with Image.open(m) as img:\n",
        "            if not img.getbbox():\n",
        "                # This is a quick check to see if the image is completely black or transparent.\n",
        "                empty_masks.append(m)\n",
        "            else:\n",
        "                non_empty_masks.append(m)\n",
        "    empty_count = int(pct_negative * len(empty_masks))\n",
        "    print(f\"Training on {len(non_empty_masks)} non-empty masks, and {empty_count} empty masks\")\n",
        "    fileset = non_empty_masks + empty_masks[:empty_count]\n",
        "    return [str(f).replace('_mask', '') for f in fileset]\n",
        "\n",
        "def get_y_seg(x_path):\n",
        "    return x_path.replace('.png', '_mask.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7GeF4bMPjy-"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Fastai DataLoaders for grayscale images\n",
        "path_seg = \"/root/work/masked_tiles\"  # Dataset's path\n",
        "ParentSplitter_seg = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
        "SAR_stats = [0.2087162, 0.13736105] # Calculated from the entire training dataset\n",
        "\n",
        "codes = ['background', 'infrastructure', 'natural', 'vessel_coincident', 'vessel_recent', 'vessel_old', 'ambiguous']\n",
        "\n",
        "cbs_seg = [WandbCallback(log_model=True),TerminateOnNaNCallback(), GradientAccumulation(8), GradientClip(), SaveModelCallback(), ShowGraphCallback()]\n",
        "#  ShortEpochCallback(pct=0.1, short_valid=False),\n",
        "# EarlyStoppingCallback(min_delta=.001, patience=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "oxMZZwiGH-tg",
        "outputId": "c4a52633-e8a1-438e-b9b7-d460a8b91512"
      },
      "outputs": [],
      "source": [
        "def get_dataloader_seg(SIZE, resize, batch_size, num_of_workers):\n",
        "\n",
        "    # Define a DataBlock and dataloaders\n",
        "    datablock_seg = DataBlock(\n",
        "        blocks= [ImageBlock(cls=PILImageBW), MaskBlock(codes = codes)], # Use ImageBlock twice for autoencoder, or (ImageBlock, Maskblock) segmentation etc.\n",
        "        get_items=get_image_files_no_masks,\n",
        "        splitter=ParentSplitter_seg,  # Split based on folder names\n",
        "        get_y=get_y_seg,  # For autoencoders, where input x is the target y. Adjust if necessary.\n",
        "        batch_tfms = aug_transforms(do_flip=True, flip_vert=True, pad_mode=PadMode.Zeros, size=(SIZE, SIZE)),\n",
        "        item_tfms = Resize(resize)\n",
        "    )\n",
        "\n",
        "    dls_seg = datablock_seg.dataloaders(path_seg, path=path_seg, bs=batch_size, num_workers=num_of_workers).to(device)\n",
        "    dls_seg.after_item = Pipeline([ToTensor(), IntToFloatTensor()])\n",
        "    dls_seg.after_batch = Pipeline([NormalizeAndClamp(*SAR_stats)])\n",
        "    return dls_seg\n",
        "\n",
        "#learn_baseline=unet_learner(dls=dls_seg, arch=resnet101, loss_func=loss_func, cbs=cbs_seg, n_in=1, lr=1e-3, wd=1e-3) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_configured_learner(dls_seg, backbone=backbone, loss_func_name=loss_func_name, cbs_seg=cbs_seg, lr=1e-3, wd=w_d):\n",
        "    loss_func = globals()[loss_func_name](ignore_index=6, axis=1)\n",
        "    if \"convnext\" in backbone:\n",
        "        model_func = globals()[backbone]\n",
        "        body = create_body(model_func(), 1, pretrained=True)\n",
        "        unet = DynamicUnet(body[0], n_out=7, img_size = (128,128))\n",
        "    return Learner(dls_seg, unet, loss_func=loss_func, cbs=cbs_seg, lr=lr, wd=wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip4Y2_7BhrUg"
      },
      "outputs": [],
      "source": [
        "def train_loop(freeze_epochs,unfreeze_epochs, progressive_resizing=progressive_resizing):\n",
        "    progressive_dls = []\n",
        "    for resize in progressive_resizing:\n",
        "        progressive_dls.append(get_dataloader_seg(final_resize_resolution, resize, batch_size, num_of_workers))\n",
        "    #freeze training\n",
        "    learner = build_configured_learner(progressive_dls[0])\n",
        "\n",
        "    for i,dls in enumerate(progressive_dls):\n",
        "        learner.dls = dls\n",
        "        fr_ep = int(freeze_epochs/len(progressive_dls))\n",
        "\n",
        "        print(\"Freezing encoder and training for\", fr_ep, \"epochs at \", progressive_resizing[i], \"input size..,\")\n",
        "        for param in learner.model[0].parameters():\n",
        "            param.requires_grad = False\n",
        "        learner.fit_one_cycle(fr_ep)\n",
        "\n",
        "    for i,dls in enumerate(progressive_dls):\n",
        "        learner.dls = dls\n",
        "        un_ep = int(unfreeze_epochs/len(progressive_dls))\n",
        "        print(\"Unfreezing encoder and training for\", un_ep, \"epochs at \", progressive_resizing[i], \"input size...\")\n",
        "        for param in learner.model[0].parameters():\n",
        "            param.requires_grad = True\n",
        "        learner.fit_one_cycle(un_ep)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loop(freeze_epochs=2, unfreeze_epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
