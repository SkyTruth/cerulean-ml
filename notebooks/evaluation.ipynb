{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "461ffad6-f899-4215-a5a8-fcb7e1a3259c",
   "metadata": {},
   "source": [
    "### Icevision Inference and Evalutation\n",
    "\n",
    "This notebook walks through the steps to load a portable torchscript model with torch and run inference on our validation set (or test set)\n",
    "\n",
    "Loading a Torchscript scripting model.\n",
    "\n",
    "Reminder: if this cell is failing, remember to mount the GCP buckets with `cdata` and `cdata2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34699e-749b-48c9-a1da-ea5dd075e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from ceruleanml.coco_stats import all_sample_stat_lists\n",
    "from ceruleanml.coco_load_fastai import record_collection_to_record_ids, get_image_path, record_to_mask\n",
    "from ceruleanml import preprocess\n",
    "from ceruleanml import data\n",
    "from icevision import models, parsers, show_records, tfms, Dataset, Metric, COCOMetric, COCOMetricType, show_records\n",
    "import numpy as np\n",
    "import skimage.io as skio\n",
    "from ceruleanml.inference import keep_by_global_pixel_nms, keep_boxes_by_idx, keep_by_bbox_score, flatten_pixel_segmentation, flatten_gt_segmentation\n",
    "from ceruleanml import data\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision #Torchvision is required to load the model, it does a global import in the background to include the NMS operation that Mask-R-CNN needs.\n",
    "print(torchvision.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916eff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ceruleanml.learner_config import (\n",
    "    run_list,\n",
    "    classes_to_keep,\n",
    "    get_tfms,\n",
    "    wd,\n",
    "    record_collection_train,\n",
    "    record_collection_val,\n",
    "    record_collection_test,\n",
    "    # record_collection_rrctrained,\n",
    "    thresholds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e26cb6-dcda-4108-8a22-da325c20fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_05_22_15_51_39_4cls_rn34_pr224_px1024_60min_maskrcnn\") # infra+density\n",
    "icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_06_03_01_20_11_4cls_rn34_pr224_px1024_60min_maskrcnn\") # infra+density3\n",
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_06_03_23_25_07_4cls_rn34_pr224_px1024_60min_maskrcnn\") # infra+density4\n",
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_06_04_15_36_57_4cls_rn34_pr224_px1024_60min_maskrcnn\") # infra+density5\n",
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_06_05_13_04_01_4cls_rn34_pr224_px1024_60min_maskrcnn\") # triplicate2\n",
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_06_07_03_59_53_4cls_rn34_pr224_px1024_60min_maskrcnn\") # infra+density6\n",
    "\n",
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_06_08_04_26_22_4cls_rn34_pr224_px1024_360min_maskrcnn\") # 360 infra+density\n",
    "icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_06_23_10_52_52_4cls_rn34_pr512_px1024_120min_maskrcnn\") # 512\n",
    "icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_09_26_18_30_13_4cls_rnxt101_pr512_px1024_600min_maskrcnn\") # DEPLOYED\n",
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_09_27_19_30_30_2cls_rnxt101_pr512_px1024_240min_maskrcnn\") # 512\n",
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_09_28_10_12_53_4cls_rnxt101_pr1024_px1024_240min_maskrcnn\") # 512\n",
    "# icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_09_29_15_03_32_4cls_rnxt101_pr512_px1024_240min_maskrcnn_roix2_intermediate\") # 512\n",
    "icevision_experiment_dir = Path(\"/root/experiments/cv2/2023_09_30_02_50_25_4cls_rnxt101_pr512_px1024_240min_intermediate_maskrcnn_roix8\") # 512\n",
    "\n",
    "scripted_model = torch.jit.load(f\"{icevision_experiment_dir}/scripting_cpu_model.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef10c4d7-8c02-4dda-8662-125b4362ce78",
   "metadata": {},
   "source": [
    "Setting up our Icevision record collection for the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16f133",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Add a number after the colon to reduce the evaluation dataset size for faster eval\n",
    "eval_record_collection = record_collection_test[:] # record_collection_test, record_collection_val, record_collection_rrctrained\n",
    "eval_ds = Dataset(eval_record_collection, get_tfms()[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f5adb9-3bb6-4792-857b-bfc031082964",
   "metadata": {},
   "source": [
    "Grabbing a test image to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e989c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows, ncols = 4, 2\n",
    "# rand_start = random.randint(0,len(eval_ds)-ncols*nrows)\n",
    "# show_records(eval_record_collection[rand_start:rand_start+ncols*nrows], ncols=ncols, class_map=classes_to_keep, display_mask=True, display_bbox=False)\n",
    "# print(\"\\n\".join([str(np.arange(rand_start+ncols*i,rand_start+ncols*(i+1))) for i in np.arange(nrows)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_idx = 4\n",
    "\n",
    "eval_record = eval_ds[eval_idx]\n",
    "eval_img = eval_record.img\n",
    "cats = eval_record.detection.labels\n",
    "\n",
    "if cats:\n",
    "    mask_sum = np.sum([np.multiply(classes_to_keep.index(cat),eval_record.detection.mask_array[i].data) for i, cat in enumerate(cats)],0)\n",
    "else:\n",
    "    mask_sum = np.zeros([1,run_list[-1][0],run_list[-1][0]])\n",
    "# show_records([eval_record],class_map=classes_to_keep, display_mask=True, display_bbox=False)\n",
    "skio.imshow_collection([eval_img[:,:,0],mask_sum[0,:,:]], interpolation=\"nearest\")\n",
    "print(f\"Ground Truth classes: {cats}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2136f-5a47-4467-be1b-d1c4efff244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(eval_img)\n",
    "# We need to convert this to a pytorch tensor before running inference with the model we loaded\n",
    "torch.Tensor(np.moveaxis(eval_img,2,0)).shape\n",
    "# And normalize the values to fall between 0 and 1\n",
    "[torch.Tensor(np.moveaxis(eval_img,2,0))/255]\n",
    "# Finally, we need to put each sample tensor in a list, the list length is the batch dimension.\n",
    "len([torch.Tensor(np.moveaxis(eval_img,2,0))/255])\n",
    "\n",
    "# maskrcnn wants a list of 3D arrays with length of list as batch size, Fastai Unet wants a 4D array with 0th dim as batch size\n",
    "losses, pred_list = scripted_model([torch.Tensor(np.moveaxis(eval_img,2,0))/255])\n",
    "\n",
    "# fastai returns a 2D array of logits. logits need to be converted to confidence probabilities. \n",
    "# Mask RCNN returns a losses array we don't use and a list of dictionaries containing detections.\n",
    "len(pred_list)\n",
    "len(pred_list[0][\"labels\"])\n",
    "\n",
    "# We can extract the first mask in the first sample's prediction and plot it by converting it to a numpy array.\n",
    "# skio.imshow(pred_list[0]['masks'][0,0,:,:].detach().cpu().numpy())\n",
    "#TODO plot a histogram of confidence scores for a pred_list\n",
    "\n",
    "# pred_list details\n",
    "# * bbox coords are not normalized. \n",
    "# * dict values are tensors until post processed with conf thresholding.\n",
    "# * length of value list indicates how many instances detected both low and high confidence\n",
    "# * Mask R-CNN mask values are not logits, they are 0 to 1 confidence probabilities. the torchscript model applies softmax unlike the fastai unet model where we do that after inference.\n",
    "# * bbox coord order is xmin, ymin, xmax, ymax, the same as icevision record collection bbox format\n",
    "\n",
    "# pred_list[0]\n",
    "\n",
    "# # scratch code\n",
    "# import matplotlib.pyplot as plt\n",
    "# test = pred_list[0]['masks'][1,0,:,:].detach().cpu().numpy().flatten()\n",
    "# test[test == 0] = np.nan\n",
    "# plt.hist(test, bins = 300,log=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2effa2b9-97ca-47f5-a53f-1e03f4db5110",
   "metadata": {},
   "source": [
    "After inference, we need to post process the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ad87a-f8db-48e6-b658-393d8f0ee07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = pred_list[0] # We only ran one image through inference\n",
    "print(f\"\\nPredicted classes before thresholds:\\n { {round(pred_list[0]['scores'][i].item(),2): classes_to_keep[l] for i,l in enumerate(pred_list[0]['labels'])}}\\n\")\n",
    "\n",
    "bbox_score_thresh = 0. or thresholds[\"bbox_score_thresh\"]\n",
    "keep = keep_by_bbox_score(pred_dict, bbox_score_thresh)\n",
    "pred_dict = keep_boxes_by_idx(pred_dict, keep)\n",
    "\n",
    "print(f\"Predicted classes after bbox_score_thresh={bbox_score_thresh}:\\n { {round(pred_dict['scores'][i].item(),2):classes_to_keep[l] for i, l in enumerate(pred_dict['labels'])} }\\n\")\n",
    "\n",
    "pixel_nms_thresh = 0. or thresholds[\"pixel_nms_thresh\"]\n",
    "keep = keep_by_global_pixel_nms(pred_dict, pixel_nms_thresh)\n",
    "pred_dict = keep_boxes_by_idx(pred_dict, keep)\n",
    "\n",
    "print(f\"Predicted classes after pixel_nms_thresh={pixel_nms_thresh}:\\n { {round(pred_dict['scores'][i].item(),2):classes_to_keep[l] for i, l in enumerate(pred_dict['labels'])} }\\n\")\n",
    "\n",
    "skio.imshow_collection([eval_img[:,:,0], *[m[0,:,:].detach().cpu().numpy() for m in pred_dict['masks']]], interpolation=\"nearest\")\n",
    "\n",
    "poly_score_thresh = 0. or thresholds[\"poly_score_thresh\"]\n",
    "semantic_mask_pred = flatten_pixel_segmentation([pred_dict], poly_score_thresh, eval_img.shape[:2])[0]\n",
    "# The output of the last thresholding step is a 2D array of classes. we use this for pixel-wise evaluation. \n",
    "\n",
    "skio.imshow_collection([eval_img[:,:,0],semantic_mask_pred, mask_sum[0,:,:]], interpolation=\"nearest\")\n",
    "print(f\"Classes after all thresholds:\\n { {int(o):classes_to_keep[o] for o in np.unique(semantic_mask_pred)} }\")\n",
    "\n",
    "print(f\"\\nGround Truth classes: {cats}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded02730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom imshow_collection function with labels\n",
    "def custom_imshow_collection_with_labels(image_collection, labels):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12,4))  # 1 row, 3 columns\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(image_collection):\n",
    "            ax.imshow(image_collection[i], interpolation=\"nearest\")\n",
    "            ax.axis('off')\n",
    "            ax.set_title(labels[i])\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Labels for the images\n",
    "labels = [\"VV\", \"Ground Truth\", \"Prediction\"]\n",
    "\n",
    "# Display the images with labels\n",
    "custom_imshow_collection_with_labels([eval_img[:,:,0], mask_sum[0,:,:], semantic_mask_pred], labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6629ff8a",
   "metadata": {},
   "source": [
    "# Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ceruleanml.learner_config import thresholds\n",
    "from ceruleanml.inference import raw_predict, reduce_preds\n",
    "\n",
    "raw_preds = raw_predict(scripted_model, eval_ds)\n",
    "\n",
    "reduced_preds = reduce_preds(\n",
    "    raw_preds,\n",
    "    bbox_score_thresh = thresholds[\"bbox_score_thresh\"], \n",
    "    pixel_score_thresh = thresholds[\"pixel_score_thresh\"],\n",
    "    pixel_nms_thresh = thresholds[\"pixel_nms_thresh\"], \n",
    "    poly_score_thresh = thresholds[\"poly_score_thresh\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIXEL CM\n",
    "from ceruleanml.inference import flatten_gt_segmentation, flatten_pixel_segmentation\n",
    "from ceruleanml.evaluation import cm_f1\n",
    "\n",
    "flat_gts = flatten_gt_segmentation(eval_ds)\n",
    "flat_preds = flatten_pixel_segmentation(reduced_preds, poly_score_thresh = thresholds[\"poly_score_thresh\"], shape=eval_ds[0].common.img.shape[:2])\n",
    "\n",
    "cm, f1 = cm_f1(\n",
    "    flat_gts,\n",
    "    flat_preds,\n",
    "    save_dir=icevision_experiment_dir, \n",
    "    normalize=None, \n",
    "    class_names=eval_ds[0].detection.class_map.get_classes(),\n",
    "    title=f\"Pixel Confusion Matrix Mask R-CNN: {icevision_experiment_dir}\"\n",
    ")\n",
    "for row in cm:\n",
    "    print(', '.join([str(v) for v in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANCE CM\n",
    "from icevision.metrics import SimpleConfusionMatrix\n",
    "# from ceruleanml.learner_config import thresholds\n",
    "from ceruleanml.evaluation import gt_pixel_dice, gt_bbox_dice\n",
    "\n",
    "cm = SimpleConfusionMatrix(\n",
    "    groundtruth_dice_thresh = thresholds[\"groundtruth_dice_thresh\"],\n",
    "    groundtruth_dice_function = gt_pixel_dice) # this class is edited on SkyTruth fork\n",
    "cm.accumulate(eval_ds, reduced_preds)\n",
    "cm.finalize()\n",
    "for row in cm.confusion_matrix:\n",
    "    print(', '.join([str(v) for v in row]))\n",
    "    \n",
    "cm.plot(figsize=5, normalize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc4db0a",
   "metadata": {},
   "source": [
    "# Upload model to Bucket for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# subprocess.run(f\"gsutil -m cp -r {icevision_experiment_dir} gs://ceruleanml/experiments/\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this SQL manually to update the database\n",
    "type_var = \"MASKRCNN\"\n",
    "layers_var = [\"VV\",\"INFRA\",\"VESSEL\"]\n",
    "cls_map_var = {0: \"BACKGROUND\", 1: \"INFRA\", 2: \"NATURAL\", 3: \"VESSEL\"}\n",
    "name_var = \"ResNext 101\"\n",
    "tile_width_m_var = 20422\n",
    "tile_width_px_var = 512\n",
    "epochs_var = 122\n",
    "backbone_size_var = 101\n",
    "pixel_f1_var = 0.434\n",
    "instance_f1_var = 0.414\n",
    "sql = f\"\"\"\n",
    "INSERT INTO model (type, file_path, layers, cls_map, name, tile_width_m, tile_width_px, epochs, thresholds, backbone_size, pixel_f1, instance_f1)\n",
    "VALUES ('{type_var}', 'experiments/{icevision_experiment_dir.name}/scripting_cpu_model.pt', ARRAY{layers_var}, '{cls_map_var}'::json, '{name_var}', {tile_width_m_var}, {tile_width_px_var}, {epochs_var}, '{thresholds}'::json, {backbone_size_var}, {pixel_f1_var}, {instance_f1_var});\n",
    "\"\"\"\n",
    "print(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcc6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for and update:\n",
    "#   cerulean-cloud-images:weigths_name: experiments/2023_09_26_18_30_13_4cls_rnxt101_pr512_px1024_600min_maskrcnn/scripting_cpu_model.pt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d50d5e-776e-4762-a8ec-2dd2bf523386",
   "metadata": {},
   "source": [
    "# Fastai Inference Evaluation with a Pixel Wise confusion matrix\n",
    "\n",
    "Like with icevision, we can run inference with a portable torchscript model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23253b65-bc5e-4cf4-979d-969fc5c19720",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_unet_experiment_dir = \"/root/data/experiments/cv2/29_Jun_2022_06_36_38_fastai_unet\"\n",
    "\n",
    "tracing_model_cpu_pth = f\"{fastai_unet_experiment_dir}/tracing_cpu_224_120__512_36__4_34_0.0003_0.436.pt\"\n",
    "\n",
    "model = torch.jit.load(tracing_model_cpu_pth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a4b36ba-ecba-4a8f-8766-2132d5f67954",
   "metadata": {},
   "source": [
    "Next, we set up the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822268e8-27cb-4bd9-a62e-0d22f7e8d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ceruleanml import data\n",
    "from ceruleanml import evaluation\n",
    "from ceruleanml import preprocess\n",
    "import os, random\n",
    "import skimage.io as skio\n",
    "import numpy as np\n",
    "from ceruleanml.coco_load_fastai import record_collection_to_record_ids, get_image_path, record_to_mask\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.vision.data import ImageBlock, MaskBlock\n",
    "from fastai.data.transforms import IndexSplitter\n",
    "from fastai.vision.augment import aug_transforms, Resize\n",
    "from ceruleanml.inference import logits_to_classes, apply_conf_threshold\n",
    "\n",
    "bs_d ={512:4, 256:32, 224:32, 128:64, 64:256}\n",
    "lr_d = {512:3e-4, 256:1e-3, 224:3e-3, 128:3e-3, 64:1e-2}\n",
    "arch_d = {18: resnet18, 34: resnet34, 50: resnet50}\n",
    "class_model_file = \"/root/experiments/cv2/26_Jul_2022_21_57_24_fastai_unet/tracing_cpu_test_32_34_224_0.824_30.pt\"\n",
    "\n",
    "### Parsing COCO Dataset with Icevision\n",
    "\n",
    "# for fastai we need the train set to parse the val set with fastai dls\n",
    "mount_path = \"/root/\"\n",
    "train_set = \"train-with-context-512\"\n",
    "tiled_images_folder_train = \"tiled_images\"\n",
    "json_name_train = \"instances_TiledCeruleanDatasetV2.json\"\n",
    "\n",
    "classes_to_remove=[\n",
    "    \"ambiguous\",\n",
    "    ]\n",
    "classes_to_remap ={\n",
    "    # \"old_vessel\": \"recent_vessel\",\n",
    "    # \"coincident_vessel\": \"recent_vessel\",\n",
    "}\n",
    "\n",
    "coco_json_path_train = f\"{mount_path}/partitions/{train_set}/{json_name_train}\"\n",
    "tiled_images_folder_train = f\"{mount_path}/partitions/{train_set}/{tiled_images_folder_train}\"\n",
    "record_collection_with_negative_small_filtered_train = preprocess.load_set_record_collection(\n",
    "    coco_json_path_train, tiled_images_folder_train, area_thresh, negative_sample_count_val, preprocess=False, \n",
    "    classes_to_remap=classes_to_remap, classes_to_remove=classes_to_remove\n",
    ")\n",
    "record_collection_with_negative_small_filtered_val = preprocess.load_set_record_collection(\n",
    "    coco_json_path_val, tiled_images_folder_val, area_thresh, negative_sample_count_val, preprocess=True,\n",
    "    classes_to_remap=classes_to_remap, classes_to_remove=classes_to_remove\n",
    ")\n",
    "\n",
    "record_ids_train = record_collection_to_record_ids(record_collection_with_negative_small_filtered_train)\n",
    "\n",
    "record_ids_val = record_collection_to_record_ids(record_collection_with_negative_small_filtered_val)\n",
    "\n",
    "assert len(set(record_ids_train)) + len(set(record_ids_val)) == len(record_ids_train) + len(record_ids_val)\n",
    "\n",
    "train_val_record_ids = record_ids_train + record_ids_val\n",
    "combined_record_collection = record_collection_with_negative_small_filtered_train + record_collection_with_negative_small_filtered_val\n",
    "\n",
    "def get_val_indices(combined_ids, val_ids):\n",
    "    return list(range(len(combined_ids)))[-len(val_ids):]\n",
    "\n",
    "### Constructing a FastAI DataBlock that uses parsed COCO Dataset from icevision parser. aug_transforms can only be used with_context=True\n",
    "\n",
    "val_indices = get_val_indices(train_val_record_ids, record_ids_val)\n",
    "\n",
    "def get_image_by_record_id(record_id):\n",
    "    return get_image_path(combined_record_collection, record_id)\n",
    "\n",
    "def get_mask_by_record_id(record_id):\n",
    "    return record_to_mask(combined_record_collection, record_id)\n",
    "\n",
    "coco_seg_dblock = DataBlock(\n",
    "        blocks=(ImageBlock, MaskBlock(codes=data.class_list)), # ImageBlock is RGB by default, uses PIL\n",
    "        get_x=get_image_by_record_id,\n",
    "        splitter=IndexSplitter(val_indices),\n",
    "        get_y=get_mask_by_record_id,\n",
    "        # batch_tfms=batch_transfms,\n",
    "        item_tfms = Resize(512),\n",
    "        n_inp=1\n",
    "    )\n",
    "\n",
    "dset = coco_seg_dblock.datasets(source= record_ids_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ab5646f-c69d-41ff-8cad-0b6059000f45",
   "metadata": {},
   "source": [
    "We can grab a record of interest from the icevision record collection to inspect inference on a single result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0e897-d0d3-4823-8fc4-fcdcea4964fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_of_interest = []\n",
    "for record in record_collection_with_negative_small_filtered_train:\n",
    "    if \"S1A_IW_GRDH_1SDV_20200724T020738_20200724T020804_033590_03E494_B457\" in str(record.common.filepath):\n",
    "        records_of_interest.append(record)\n",
    "\n",
    "idx_of_interest = records_of_interest[6].common.record_id\n",
    "\n",
    "idx = train_val_record_ids.index(idx_of_interest)\n",
    "\n",
    "img, mask = dset[idx]\n",
    "\n",
    "%matplotlib inline\n",
    "skio.imshow(np.array(img)[:,:,0])\n",
    "img = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b18066-552e-4b57-8f26-48ccbd8ce6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2b73a2-633c-44c8-aa5b-6e7385720edc",
   "metadata": {},
   "source": [
    "Like inputs to icevision, we need to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9f102-eb33-423b-96aa-ae6627111272",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_arr = normalize(torch.Tensor(np.moveaxis(img, 2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b385242-6483-4ab2-b87c-9b71c3eaf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(norm_arr[0,:,:].cpu().detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af6bcf09-0275-4f71-ae7b-2a33ee286540",
   "metadata": {},
   "source": [
    "And we can then run prediction like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44ac63-32e6-45a0-8bb6-af0b911dfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = model(norm_arr.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d141f3-974f-4cde-b271-485f29469650",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec252c9-c44a-489e-a523-ac7cad2f8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs,classes = logits_to_classes(pred_arr)\n",
    "\n",
    "dice_table = apply_conf_threshold(probs, classes, .4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f432e4-4700-4312-9f0a-f129995b2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_conf_threshold??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_table.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "636c139e-5cc6-4bbc-81b8-c48a30cce5f5",
   "metadata": {},
   "source": [
    "The result from Fastai after confidence thresholding is a 2D array/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61663a-fda7-457f-bb3d-9e9090f1bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(dice_table.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c53bb0-a7f1-460f-8f14-1e75b5ce869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dice_table.detach().cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57a3723c-46e2-4c8f-9d09-dbec26cb5c32",
   "metadata": {},
   "source": [
    "## Confusion Matrix Comparison for Unet and MaskRCNN\n",
    "\n",
    "In this section we create and compare pixel-wise confusion matrices and instance-wise confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26748d0-0420-46fc-b5b3-23f59582470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ceruleanml.evaluation import get_cm_for_torchscript_model_unet, get_cm_for_torchscript_model_mrcnn\n",
    "from ceruleanml import data\n",
    "from icevision.metrics.confusion_matrix import SimpleConfusionMatrix\n",
    "# from icevision.models.checkpoint import model_from_checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75ec3262-d403-4000-b9c5-01524359a589",
   "metadata": {},
   "source": [
    "We create a fastai dls from the dataset we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014782fb-d865-4da5-975d-3b6b7c7d30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = coco_seg_dblock.dataloaders(source=train_val_record_ids, batch_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5ee5d26-5b6a-4adf-ace4-81073d842b3c",
   "metadata": {},
   "source": [
    "And then run inference and create the cm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a35a838-1eee-4b06-b89c-420622d5ab5e",
   "metadata": {},
   "source": [
    "TODO Fastai Unet CM needs to be inspected, result shows no true labels for vessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5e61f-520a-4ebd-b648-257dc79a74a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm_unet, f1_unet = get_cm_for_torchscript_model_unet(dls, model, fastai_unet_experiment_dir, semantic_mask_conf_thresh=.5, class_names=data.class_list, normalize = None, title=\"Fastai Unet Confusion Matrix: 29_Jun_2022_06_36_38\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26322ecb-80a3-4b5c-a5d1-6e6d8e5df557",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_unet, f1_unet = get_cm_for_torchscript_model_unet(dls, model, fastai_unet_experiment_dir, semantic_mask_conf_thresh=.5, class_names=data.class_list, normalize = \"true\", title=\"Fastai Unet Confusion Matrix: 29_Jun_2022_06_36_38\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1db5773-f28b-44b9-9ce5-a23e880e457c",
   "metadata": {},
   "source": [
    "The pixel wise mrcnn cm is correct. TODO this doesn't work with negative samples, only the instance confusion matrix does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051234f-c28c-43f0-a475-3cf7b5bb5831",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_cm_for_torchscript_model_mrcnn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_tfms = get_tfms()\n",
    "test_ds = Dataset(record_collection_test[:9], test_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32642079-8e1a-4eb2-bcf2-160e785d26ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm_mrcnn, f1_mrcnn = get_cm_for_torchscript_model_mrcnn(\n",
    "    test_ds, \n",
    "    scripted_model, \n",
    "    save_path=icevision_experiment_dir, \n",
    "    mask_conf_threshold=.1, \n",
    "    bbox_conf_threshold=.1, \n",
    "    soft_dice_nms_threshold=.5, \n",
    "    normalize=None, \n",
    "    title=f\"Torchvision MaskR-CNN Confusion Matrix: {icevision_experiment_dir}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2dbbc-f7d0-48c0-9965-5a415e8f2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_mrcnn, f1_mrcnn = get_cm_for_torchscript_model_mrcnn(\n",
    "    valid_ds, scripted_model, save_path=icevision_experiment_dir, mask_conf_threshold=.01, bbox_conf_threshold=.7, normalize=\"true\", class_names=data.class_list, title=\"Torchvision MaskR-CNN Confusion Matrix: 20_Jul_2022_00_14_15\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b50c4-eef7-4e83-9615-288680ea88cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/root/data/experiments/cv2/20_Jul_2022_00_14_15_icevision_maskrcnn/state_dict_test_28_34_224_58.pt'\n",
    "\n",
    "checkpoint_and_model = model_from_checkpoint(checkpoint_path, \n",
    "    model_name='torchvision.mask_rcnn', \n",
    "    backbone_name='resnet34_fpn',\n",
    "    img_size=224, \n",
    "    classes=data.class_list,\n",
    "    is_coco=False)\n",
    "\n",
    "model = checkpoint_and_model[\"model\"]\n",
    "model_type = checkpoint_and_model[\"model_type\"]\n",
    "backbone = checkpoint_and_model[\"backbone\"]\n",
    "class_map = checkpoint_and_model[\"class_map\"]\n",
    "img_size = checkpoint_and_model[\"img_size\"]\n",
    "model_type, backbone, class_map, img_size\n",
    "\n",
    "infer_dl = model_type.infer_dl(valid_ds, batch_size=1,shuffle=False)\n",
    "\n",
    "preds = model_type.predict_from_dl(model, infer_dl, keep_images=True, detection_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a960d34-8ead-429e-a5c4-83cd230a3565",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "__all__ = [\"COCOMetric\", \"COCOMetricType\"]\n",
    "\n",
    "from icevision.imports import *\n",
    "from icevision.utils import *\n",
    "from icevision.data import *\n",
    "from icevision.metrics.metric import *\n",
    "\n",
    "\n",
    "class COCOMetricType(Enum):\n",
    "    \"\"\"Available options for `COCOMetric`.\"\"\"\n",
    "\n",
    "    bbox = \"bbox\"\n",
    "    mask = \"segm\"\n",
    "    keypoint = \"keypoints\"\n",
    "\n",
    "\n",
    "class COCOMetric(Metric):\n",
    "    \"\"\"Wrapper around [cocoapi evaluator](https://github.com/cocodataset/cocoapi)\n",
    "\n",
    "    Calculates average precision.\n",
    "\n",
    "    # Arguments\n",
    "        metric_type: Dependent on the task you're solving.\n",
    "        print_summary: If `True`, prints a table with statistics.\n",
    "        show_pbar: If `True` shows pbar when preparing the data for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metric_type: COCOMetricType = COCOMetricType.bbox,\n",
    "        iou_thresholds: Optional[Sequence[float]] = None,\n",
    "        print_summary: bool = False,\n",
    "        show_pbar: bool = False,\n",
    "    ):\n",
    "        self.metric_type = metric_type\n",
    "        self.iou_thresholds = iou_thresholds\n",
    "        self.print_summary = print_summary\n",
    "        self.show_pbar = show_pbar\n",
    "        self._records, self._preds = [], []\n",
    "\n",
    "    def _reset(self):\n",
    "        self._records.clear()\n",
    "        self._preds.clear()\n",
    "\n",
    "    def accumulate(self, preds):\n",
    "        for pred in preds:\n",
    "            self._records.append(pred.ground_truth)\n",
    "            self._preds.append(pred.pred)\n",
    "\n",
    "    def finalize(self) -> Dict[str, float]:\n",
    "        with CaptureStdout():\n",
    "            coco_eval = create_coco_eval(\n",
    "                records=self._records,\n",
    "                preds=self._preds,\n",
    "                metric_type=self.metric_type.value,\n",
    "                iou_thresholds=self.iou_thresholds,\n",
    "                show_pbar=self.show_pbar,\n",
    "            )\n",
    "            coco_eval.evaluate()\n",
    "            coco_eval.accumulate()\n",
    "\n",
    "        with CaptureStdout(propagate_stdout=self.print_summary):\n",
    "            coco_eval.summarize()\n",
    "\n",
    "        stats = coco_eval.stats\n",
    "        logs = {\n",
    "            \"AP (IoU=0.50:0.95) area=all\": stats[0],\n",
    "            \"AP (IoU=0.50) area=all\": stats[1],\n",
    "            \"AP (IoU=0.75) area=all\": stats[2],\n",
    "            \"AP (IoU=0.50:0.95) area=small\": stats[3],\n",
    "            \"AP (IoU=0.50:0.95) area=medium\": stats[4],\n",
    "            \"AP (IoU=0.50:0.95) area=large\": stats[5],\n",
    "            \"AR (IoU=0.50:0.95) area=all maxDets=1\": stats[6],\n",
    "            \"AR (IoU=0.50:0.95) area=all maxDets=10\": stats[7],\n",
    "            \"AR (IoU=0.50:0.95) area=all maxDets=100\": stats[8],\n",
    "            \"AR (IoU=0.50:0.95) area=small maxDets=100\": stats[9],\n",
    "            \"AR (IoU=0.50:0.95) area=medium maxDets=100\": stats[10],\n",
    "            \"AR (IoU=0.50:0.95) area=large maxDets=100\": stats[11],\n",
    "        }\n",
    "\n",
    "        self._reset()\n",
    "        return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc481bb-c45b-4842-af98-2966fcaa540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0].pred.detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf73b6-8bc7-42ac-bec4-329bd86fbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0].pred.detection.bboxes[0].xywh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244278e1-1cba-4a1a-88f4-fc924fb77904",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = SimpleConfusionMatrix() # this class is edited on SkyTruth fork\n",
    "cm.accumulate(preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cbbc4cc",
   "metadata": {},
   "source": [
    "TODO highlight edits to icevision cm code and how to and where to install this edited icevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4763feb-892b-4758-bce7-261ebf62919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "_ = cm.finalize()\n",
    "\n",
    "cm.plot(figsize=5, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b62a01-6bbd-4a1a-a1c3-90b23427e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.class_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.ice-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0cd103e2f87710db3c13fdf2c1f4c98edf2a6212166ef41cb835fc40fe5ebcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
