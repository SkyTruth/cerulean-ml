{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[1mINFO    \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m67\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n"
     ]
    }
   ],
   "source": [
    "from ceruleanml import data\n",
    "from icevision.parsers import COCOMaskParser\n",
    "from icevision.data import SingleSplitSplitter\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.vision.data import ImageBlock, MaskBlock\n",
    "from fastai.vision.augment import aug_transforms\n",
    "from fastai.vision.learner import unet_learner\n",
    "from fastai.data.transforms import RandomSplitter, Normalize\n",
    "from fastai.metrics import Dice\n",
    "from ceruleanml.coco_load_fastai import record_collection_to_record_ids, get_image_path, record_to_mask\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing COCO Dataset with Icevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/root/\"\n",
    "mount_path = \"/root/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2c9c06dd1348e1acf2806af4c7f846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_map = {v: k for k, v in data.class_mapping_coco_inv.items()}\n",
    "class_ints = list(range(1, len(list(class_map.keys())[:-1]) + 1))\n",
    "parser = COCOMaskParser(annotations_filepath=f\"{data_path}/tile-cerulean-v2-partial-with-context/instances_Tiled Cerulean Dataset V2.json\", img_dir=f\"{mount_path}/tile-cerulean-v2-partial-with-context/tiled_images\")\n",
    "train_records, valid_records = parser.parse(autofix=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing functions for returning an image sample and a semantic segmentation label for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_ids = record_collection_to_record_ids(train_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a FastAI DataBlock that uses parsed COCO Dataset from icevision parser and applies transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_by_record_id(record_id):\n",
    "    return get_image_path(train_records, record_id)\n",
    "def get_mask_by_record_id(record_id):\n",
    "    return record_to_mask(train_records, record_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [60.73,       190.3,      4.3598]\n",
    "std = [16.099,      17.846,       9.603]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transfms = [aug_transforms(),  Normalize.from_stats(mean,std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/_tensor.py:1051: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#size = 64  # Progressive resizing could happen here\n",
    "augs = aug_transforms(flip_vert=True, max_warp=0.1) #, size=size)\n",
    "coco_seg_dblock = DataBlock(\n",
    "    blocks=(ImageBlock, MaskBlock(codes=class_ints)),\n",
    "    get_x=get_image_by_record_id,\n",
    "    splitter=RandomSplitter(),\n",
    "    get_y=get_mask_by_record_id,\n",
    "    batch_tfms=[Normalize.from_stats(mean,std)],\n",
    "    n_inp=1,\n",
    ")\n",
    "\n",
    "dls = coco_seg_dblock.dataloaders(source=record_ids, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from [30477, 66481, 15704, 33144, 49895, 22086, 31863, 63334, 51044, 31480, 54136, 30504, 77054, 54822, 3115, 32915, 31024, 19683, 47035, 15826, 19682, 55872, 5276, 51749, 29087, 7293, 44622, 24519, 56022, 67646, 19844, 8076, 54279, 25627, 49696, 49693, 24870, 31937, 28516, 42301, 27093, 70866, 68064, 39139, 23451, 66082, 37502, 19667, 46713, 18865, 77673, 53899, 25100, 35504, 17075, 64430, 22901, 21465, 53250, 33273, 27649, 44013, 5419, 34301, 16425, 44029, 22440, 51270, 43292, 74108, 62090, 58729, 30723, 11707, 54643, 3130, 31330, 22492, 19827, 42236, 18133, 15720, 64254, 20665, 27622, 32476, 11008, 7279, 72481, 13281, 29018, 37925, 74293, 45840, 31279, 34729, 65017, 46360, 34254, 22084, 47701, 37924, 18684, 55886, 26836, 72468, 11874, 36916, 36500, 64651, 77674, 9420, 10881, 55472, 37853, 44056, 26264, 26707, 59110, 68275, 8659, 49424, 65818, 22100, 65031, 36523, 20474, 13037, 50470, 28, 23100, 66095, 63063, 8091, 47019, 38258, 16442, 75879, 21234, 58473, 22460, 67459, 4902, 71020, 75636, 37487, 36544, 29272, 69933, 63062, 55079, 64650, 32860, 44057, 49676, 54111, 23094, 45854, 10492, 56262, 13119, 20651, 15653, 25448, 40524, 73888, 29708, 20286, 65664, 47072, 26072, 21691, 15067, 27898, 25904, 39294, 27537, 68104, 31135, 45661, 50038, 33114, 25630, 54833, 41919, 50834, 69315, 14094, 77254, 60034, 51016, 20272, 62330, 38435, 6070, 34832, 60898, 42230, 16902, 46688, 76634, 23068, 68090, 63306, 65666, 28337, 37103, 28624, 58304, 42235, 25702, 30696, 48045, 73249, 45676, 16267, 42642, 11724, 6067, 43318, 31671, 4888, 74435, 37811, 20300, 19301, 67632, 20504, 27036, 32479, 32873, 28073, 70634, 36718, 55639, 5647, 24647, 49707, 72268, 25658, 14514, 56090, 68901, 21664, 68679, 8517, 4818, 4466, 19637, 41920, 68435, 60288, 36719, 19300, 55518, 70864, 57499, 49894, 23675, 37519, 76235, 77508, 25860, 41722, 25677, 55857, 27272, 59224, 55473, 25645, 74844, 65859, 21263, 50933, 38111, 70085, 22676, 72038, 25845, 55503, 63473, 58744, 19081, 72529, 55094, 37732, 17307, 51247, 64647, 76109, 35247, 59456, 4419, 47895, 64418, 30273, 60437, 33143, 2903, 36903, 41720, 22442, 43488, 46658, 17868, 6061, 67886, 1609, 16426, 5435, 20432, 24503, 71907, 54109, 64649, 77253, 67703, 66467, 38921, 12027, 3506, 71034, 19652, 68647, 66277, 17103, 41679, 74109, 22098, 22296, 21232, 15504, 76220, 32256, 30243, 76664, 66929, 5098, 17850, 31133, 18233, 20480, 24885, 59868, 20631, 75437, 42284, 30680, 72053, 33909, 53101, 4048, 62331, 23623, 11266, 5101, 45824, 32644, 49694, 69078, 28625, 15503, 19651, 4858, 6251, 22491, 21267, 29423, 30229, 37716, 65478, 32243, 63322, 38084, 30247, 6516, 47490, 75249, 38509, 54836, 25703, 17857, 54093, 62485, 33691, 15827, 24254, 33619, 66913, 19317, 37119, 52018, 60899, 70063, 24043, 48826, 27897, 27882, 27286, 25104, 23651, 9297, 39280, 22280, 19685, 28528, 8330, 46674, 25686, 13237, 33924, 34269, 13280, 35249, 61652, 62075, 23467, 31293, 37718, 75423, 3521, 68445, 38224, 60289, 57701, 222, 54038, 59854, 15068, 3876, 71617, 44621, 30237, 56276, 70485, 4404, 25718, 54249, 70671, 40874, 74452, 70319, 30859, 2038, 52924, 31633, 32257, 56437, 34030, 49679, 17873, 72487, 33129, 49091, 23277, 57086, 18282, 34847, 2022, 69512, 32063, 68836, 37926, 73128, 64679, 19080, 65665, 58306, 37117, 62865, 68276, 66881, 64072, 37233, 20460, 11306, 28837, 29709, 61302, 45632, 20679, 8861, 32662, 51256, 12659, 25458, 76066, 27881, 40036, 22885, 55679, 9068, 38243, 63903, 40052, 47908, 21481, 16887, 29227, 25077, 17680, 12044, 34014, 17500, 54686, 69700, 70631, 47260, 28295, 22863, 68151, 35127, 22069, 54879, 67885, 25292, 40277, 55678, 32914, 64117, 34531, 14883, 36918, 64087, 30679, 629, 68867, 56625, 58036, 48536, 65704, 12317, 30221, 53251, 4287, 2080, 25473, 54234, 10101, 15737, 57503, 61336, 13890, 19462, 22475, 33690, 32685, 3098, 52668, 26672, 31134, 59452, 55826, 54657, 72485, 67502, 30695, 17837, 41082, 12871, 18886, 22900, 33132, 18682, 64405, 17853, 49089, 22833, 22477, 70847, 37530, 12834, 25613, 57085, 31631, 70332, 19684, 27051, 21822, 49710, 19464, 72276, 27035, 33272, 7280, 22873, 8892, 7861, 75047, 50052, 63032, 54135, 25422, 30874, 70686, 55532, 54832, 55515, 67702, 51222, 30520, 62095, 77520, 30230, 68049, 75865, 56465, 37733, 45848, 4721, 66914, 48210, 32932, 35723, 70084, 59699, 33507, 77255, 27634, 57856, 66945, 49632, 69045, 27054, 11252, 29242, 56851, 35219, 30222, 31659, 44028, 75635, 59713, 72501, 43020, 55694, 27097, 53918, 73235, 25293, 36486, 4314, 62693, 17872, 59468, 30249, 32493, 73655, 62301, 5110, 50510, 34046, 68884, 63294, 2255, 15721, 70318, 12831, 30493, 69513, 12021, 16268, 62710, 20632, 31237, 14528, 70724, 66260, 70655, 20087, 61434, 51045, 32845, 47715, 40293, 2639, 33657, 2681, 14527, 18849, 68434, 52034, 33117, 25719, 64666, 23866, 29942, 41041, 42845, 48924, 4288, 57487, 33228, 29710, 13222, 34031, 38906, 18119, 68136, 41122, 74306, 32286, 28294, 35873, 59700, 30234, 32874, 13908, 64241, 52312, 54701, 69529, 52032, 61304, 9450, 45868, 17834, 54672, 37702, 47476, 44019, 4285, 49451, 57097, 18669, 39430, 19862, 43507, 37248, 11254, 35874, 62678, 25306, 24031, 25661, 30231, 2653, 41708, 77509, 24030, 57715, 49422, 2710, 28610, 25708, 38239, 24869, 65266, 8904, 27316, 32506, 41932, 59853, 12025, 75265, 11253, 24703, 48224, 32496, 28668, 57096, 77519, 73672, 55500, 4300, 53412, 59404, 8285, 76306, 69545, 75050, 30233, 59840, 6502, 54120, 75217, 27509, 835, 73234, 74434, 26822, 63348, 31246, 54849, 48268, 72901, 3231, 30223, 34048, 69318, 49118, 54263, 24253, 18681, 58446, 25307, 29115, 34715, 15053, 66291, 55299, 65832, 42501, 7856, 61436, 49231, 77506, 30245, 6884, 54894, 44242, 25628, 15054, 28334, 29426, 48047, 39464, 17854, 37923, 72082, 30101, 35904, 2236, 23070, 35130, 54022, 11212, 51720, 19243, 50916, 34819, 47462, 17088, 70879, 35099, 32094, 61306, 26695, 42339, 23276, 33656, 1868, 58317, 72657, 62500, 5112, 14663, 63320, 64059, 27620, 18905, 21476, 33692, 25691, 72517, 65286, 72052, 33116, 41948, 49687, 19479, 18120, 59839, 54121, 29446, 1865, 34532, 23468, 45635, 25644, 68648, 47245, 59234, 75471, 56481, 30490, 29448, 28543, 28087, 55888, 50917, 28308, 36919, 71895, 50918, 5620, 61305, 65492, 66508, 31661, 58064, 31247, 36035, 36705, 37218, 6250, 17323, 8888, 39517, 12299, 45629, 61439, 55317, 34730, 12413, 37687, 30849, 39518, 72530, 66931, 70880, 13514, 40649, 50641, 26835, 28823, 3466, 74305, 29640, 75237, 22085, 67520, 69062, 34831, 69317, 645, 53402, 33130, 43019, 9821, 3859, 17851, 35246, 33641, 64431, 71908, 21677, 49216, 33623, 19272, 45849, 16443, 57504, 33689, 19828, 66898, 31892, 29086, 49646, 11230, 26420, 22915, 47731, 54265, 74321, 52519, 15530, 29, 18487, 30861, 9668, 36687, 33639, 7292, 26431, 7096, 66894, 29652, 42317, 26071, 45634, 62499, 25462, 56464, 35234, 27895, 23484, 15670, 27288, 16696, 58292, 16423, 64073, 50884, 49896, 26723, 67899, 71281, 56639, 27477, 77687, 36887, 21088, 37220, 5261, 71517, 36365, 7862, 58274, 66480, 57700, 68319, 1284, 31235, 25659, 10897, 28100, 15055, 41287, 34013, 61672, 23259, 67487, 50637, 19512, 55681, 63049, 37118, 38907, 70702, 35887, 70333, 54043, 18693, 34268, 73250, 30248, 31621, 426, 36889, 34253, 47504, 20489, 19699, 26131, 57823, 7871, 48825, 57500, 19843, 40881, 20447, 23066, 4272, 31465, 61695, 39110, 34514, 25423, 27690, 65845, 34685, 38631, 50657, 53076, 19671, 32242, 49633, 68422, 62107, 9835, 26450, 14632, 37204, 4302, 52233, 48535, 55680, 51245, 16285, 20503, 70022, 51257, 13515, 28838, 76110, 12833, 4844, 8302, 30863, 626, 23619, 20462, 54908, 54092, 28824, 8300, 73142, 74277, 30481, 28306, 74433, 72515, 74920, 30251, 61454, 34829, 13839, 43903, 31039, 70896, 64253, 62866, 43904, 68118, 14122, 3450, 31044, 62094, 66947, 37704, 58260, 72486, 24632, 19066, 27101, 65690, 29916, 68837, 30071, 51249, 67872, 57872, 76435, 35918, 43834, 50100, 5605, 72916, 8301, 27083, 34285, 76094, 31645, 32021, 23638, 32900, 62089, 37954, 53901, 71603, 36048, 25463, 36930, 27047, 33473, 75523, 2682, 37909, 77069, 75053, 63336, 50053, 25299, 19670, 33908, 73887, 10299, 20446, 47231, 71503, 72484, 21466, 58288, 7474, 60623, 19045, 36034, 34529, 72900, 37130, 65030, 54656, 63285, 20278, 56108, 13823, 51261, 13530, 27691, 6699, 37533, 52940, 67818, 27130, 35233, 13118, 58303, 25687, 31662, 56924, 52505, 5888, 19463, 3114, 70516, 28667, 66110, 29493, 53898, 46034, 3451, 41949, 59123, 6068, 28323, 53289, 66523, 73220, 20488, 3683, 75451, 70862, 69528, 75048, 17483, 857, 63708, 23690, 61273, 19316, 23052, 37673, 31262, 12029, 75483, 66915, 19838, 12818, 73624, 20459, 22691, 31649, 18489, 32887, 43317, 62694, 52019, 16117, 11890, 1018, 55695, 54424, 37102, 22493, 8221, 45314, 42639, 36515, 19257, 25447, 72915, 49450, 72067, 23882, 65706, 30845, 43463, 51235, 12430, 9291, 19686, 41678, 39094, 73205, 65253, 74921, 76649, 23275, 60427, 10055, 71268, 48819, 7625, 26465, 25918, 34541, 50011, 65032, 26706, 20494, 14096, 24633, 67660, 3100, 11444, 60913, 2473, 37503, 61657, 48266, 67675, 23466, 31294, 14123, 61909, 14136, 66081, 22913, 35888, 75051, 77532, 76234, 26300, 36933, 29424, 42220, 49425, 32717, 27663, 75264, 51734, 31051, 53917, 61437, 77510, 95, 54075, 63275, 18132, 7490, 14220, 56039, 9424, 8878, 67898, 37250, 8531, 46345, 62345, 72532, 24851, 71035, 55858, 20895, 51438, 20647, 3841, 72277, 30873, 25676, 48733, 7857, 37088, 33476, 76844, 48046, 77516, 51835, 22266, 77521, 37473, 50039, 20431, 35141, 30232, 74768, 55076, 41917, 24022, 76633, 68852, 22426, 64664, 11231, 72670, 33292, 62879, 31648, 64869, 41694, 66900, 23494, 12856, 56480, 53664, 6265, 41055, 24240, 30235, 11286, 31233, 54027, 62515, 25690, 8265, 19044, 72283, 14633, 1893, 8514, 20650, 23258, 25629, 25671, 26057, 70662, 34818, 68833, 53110, 35889, 53236, 16697, 25674, 2239, 34857, 54264, 8496, 61318, 5087, 69027, 60428, 27538, 26464, 24853, 75637, 3096, 25091, 2457, 33427, 1892, 30087, 68853, 23051, 75419, 31466, 54441, 25662, 66713, 27063, 68031, 23867, 9437, 20085, 20263, 4859, 32646, 3097, 67661, 55517, 61334, 74875, 57839, 70470, 68900, 65817, 5246, 24867, 25487, 64654, 50899, 18729, 58291, 15039, 75252, 3520, 64868, 55264, 48894, 35700, 29929, 21101, 77518, 76648, 49273, 33923, 49287, 40648, 73431, 54834, 27675, 27496, 72494, 38935, 53665, 23109, 75037, 34060, 37234, 6501, 66262, 75469, 30224, 69316, 42103, 39336, 61321, 25050, 13235, 36931, 32507, 68290, 65258, 35724, 31635, 43634, 12022, 24518, 50041, 23883, 2667, 14110, 3860, 73090, 2917, 35117, 34744, 39637, 23865, 71634, 43635, 16136, 23083, 61658, 21118, 72516, 77068, 36904, 59453, 37719, 16886, 44240, 36900, 47246, 74291, 57482, 51432, 71253, 39351, 10315, 30296, 51693, 36733, 69903, 49066, 73107, 26279, 11443, 40308, 33915, 76237, 62880, 64443, 67458, 43622, 52670, 70007, 27510, 44510, 2441, 54059, 65506, 22108, 27525, 25834, 22094, 69028, 34062, 17624, 49245, 63681, 76650, 30117, 62713, 34724, 35090, 75049, 21491, 33910, 75453, 4017, 9038, 31050, 37689, 70863, 70291, 66274, 20277, 32062, 30837, 54110, 76864, 60032, 22476, 31023, 56853, 51679, 4690, 35933, 48252, 71618, 17867, 65692, 7282, 51431, 28839, 34068, 73115, 33229, 39079, 32494, 70672, 37952, 30072, 60912, 17849, 75435, 22878, 50639, 56056, 20069, 21132, 57444, 76863, 23067, 36702, 19694, 45862, 77507, 25706, 73106, 21493, 27331, 29440, 33673, 40068, 11229, 37236, 27078, 65639, 33640, 42846, 20287, 31646, 5102, 29454, 411, 26246, 39322, 20476, 32109, 8236, 13531, 24252, 21479, 64682, 52672, 60443, 77686, 67882, 11250, 12034, 3277, 29928, 17060, 19698, 21848, 72083, 13840, 57857, 62711, 22461, 69917, 56923, 62698, 18883, 29681, 71049, 38083, 58035, 73135, 13873, 35221, 3113, 26432, 37133, 37101, 34061, 28836, 9296, 66522, 22914, 25120, 15722, 47909, 13824, 56852, 43304, 55887, 44, 38450, 48923, 33257, 63286, 18067, 3278, 38451, 25646, 36530, 11268, 22427, 61108, 74307, 69686, 57498, 37074, 29638, 76472, 16284, 61667, 69530, 38920, 29915, 44606, 72097, 67617, 49659, 72493, 49423, 31660, 63439, 38082, 49067, 52279, 76877, 32480, 55650, 8532, 68649, 55873, 55902, 76061, 72686, 59106, 70501, 17643, 3436, 12297, 46729, 37104, 12318, 5099, 68152, 53090, 9455, 18502, 43464, 76841, 61322, 16871, 15305, 71633, 36022, 1880, 21704, 38645, 77672, 77517, 6046, 64665, 29085, 67519, 4299, 33291, 30262, 71906, 25611, 23639, 24504, 61435, 43863, 22847, 29100, 30518, 60285, 12872, 55282, 46346, 36917, 22888, 17852, 28530, 75251, 9441, 29467, 31647, 32920, 76108, 61922, 22494, 20314, 67473, 25832, 25064, 25105, 55874, 28542, 27883, 9653, 31673, 7111, 4315, 4248, 57483, 25876, 42643, 8515, 37730, 11425, 61319, 12819, 5292, 8317, 49651, 76857, 4722, 65272, 205, 70093, 32913, 67865, 22052, 36019, 66883, 20273, 20506, 1266, 23665, 33492, 55697, 64058, 22899, 26430, 10255, 4286, 17306, 55871, 5233, 62302, 17087, 2066, 54818, 2638, 37922, 44041, 4062, 29434, 54231, 27095, 30287, 59288, 16424, 67291, 32919, 57854, 32859, 8889, 59712, 69514, 68304, 22109, 37222, 43021, 18488, 64417, 64057, 58445, 29226, 23084, 21233, 21821, 45892, 47716, 22281, 32933, 48225, 427, 18730, 47232, 63290, 26278, 55903, 20461, 27491, 44867, 57855, 8315, 8860, 35715, 29466, 65252, 31038, 64669, 70630, 43673, 75468, 21495, 38499, 63695, 70731, 31658, 34284, 48522, 73120, 44524, 67618, 25098, 67521, 33914, 56626, 38484, 52669, 37087, 46911, 64102, 18694, 33260, 1004, 68421, 66916, 32481, 25709, 12033, 63065, 3101, 64648, 73898, 24036, 58521, 35489, 61320, 32491, 37205, 29019, 64667, 31030, 39308, 38630, 14500, 38452, 37953, 21650, 71602, 17067, 31053, 47204, 53102, 58050, 42641, 77039, 31344, 34300, 24894, 15529, 12855, 54893, 27896, 30862, 27039, 11424, 14234, 45839, 11271, 70716, 43291, 44035, 39901, 4259, 72291, 4479, 73447, 63694, 42089, 54700, 30489, 73121, 31674, 11248, 27287, 56107, 11251, 7877, 38493, 18668, 45620, 43848, 24225, 29004, 49708, 35114, 21496, 30100, 15546, 38069, 12298, 31686, 47717, 14095, 45690, 48267, 15846, 47016, 4819, 8266, 16901, 24037, 45675, 19861, 75648, 49690, 50248, 28307, 18866, 73092, 30102, 26117, 3874, 65691, 35132, 75634, 60033, 17061, 41054, 64652, 68680, 53067, 72669, 24883, 23868, 43623, 22458, 63680, 29271, 29435, 24042, 10493, 39279, 17059, 19668, 30286, 18692, 55091, 59257, 69044, 46436, 67472, 45516, 32901, 13220, 17871, 14109, 69010, 2260, 36905, 43877, 72274, 58856, 25820, 43890, 10616, 31891, 65259, 4467, 72502, 54862, 63047, 48833, 66882, 34862, 62514, 19064, 22425, 31037, 33275, 63456, 42251, 35260, 25063, 51018, 31064, 7110, 6700, 56423, 61109, 34861, 11272, 24884, 64255, 3293, 28101, 67111, 54127, 28529, 31304, 51735, 23069, 72544, 2902, 33491, 68050, 20086, 34528, 66884, 76236, 19824, 19271, 73656, 19825, 56277, 850, 27635, 33674, 75434, 41918, 73104, 6900, 34686, 69011, 16444, 33457, 64444, 29253, 37132, 33675, 30521, 77531, 2680, 57838, 12036, 21494, 68105, 61123, 69684, 17679, 12431, 29653, 42300, 65029, 42266, 2254, 29930, 35113, 36875, 52491, 56075, 66820, 66288, 69061, 74276, 67488, 35708, 70895, 44852, 32222, 73671, 15036, 22459, 22675, 26837, 32918, 22083, 59663, 67290, 34517, 68274, 30256, 206, 630, 30492, 64419, 75484, 69916, 49688, 39636, 31063, 25614, 27676, 60286, 8890, 3465, 3246, 69902, 45689, 37100, 11267, 25322, 71295, 30503, 6865, 21705, 26287, 7296, 39109, 69918, 29422, 30057, 45615, 25610, 32699, 1819, 29238, 68137, 29099, 34527, 32223, 45904, 62697, 76253, 70078, 36704, 10256, 32899, 75524, 4047, 63905, 46923, 14515, 45040, 34848, 14648, 26260, 29439, 53088, 5874, 49217, 44657, 25626, 53897, 27476, 33141, 59469, 69497, 31105, 31622, 65678, 72658, 1867, 37073, 75052, 4478, 5111, 29237, 21820, 35235, 26433, 50640, 18715, 70069, 22690, 75880, 20633, 66289, 29694, 14108, 21849, 58728, 34530, 60284, 11287, 65505, 76065, 23452, 27062, 70717, 31623, 70865, 76233, 46897, 2665, 19418, 63048, 50883, 59686, 21248, 24914, 17431, 68678, 49637, 50496, 50471, 59125, 42640, 12832, 67292, 20068, 51439, 8511, 76062, 43022, 31251, 69531, 48269, 44624, 9438, 49318, 17838, 4334, 48732, 66712, 11210, 72275, 72463, 38919, 76433, 47730, 11249, 42252, 67503, 59287, 8831, 48910, 19858, 43278, 18116, 17499, 36703, 39915, 22428, 75036, 45838, 50511, 47687, 4061, 55856, 45039, 10102, 50012, 4707, 11009, 9426, 30058, 68444, 44509, 68869, 71267, 76083, 16286, 21074, 54837, 52033, 47218, 29252, 40665, 26830, 23663, 75438, 2666, 73218, 39916, 33475, 37219, 21817, 20102, 58520, 37131, 51433, 51692, 3858, 27673, 66851, 56479, 76290, 8495, 76249, 67518, 14121, 19478, 28072, 76843, 37517, 44039, 7291, 50901, 11709, 28873, 28874, 22441, 52939, 5100, 64268, 59256, 47020, 25672, 17888, 2235, 10896, 46438, 45869, 31278, 15636, 7094, 28822, 40070, 71296, 31052, 39870, 16118, 26831, 21699, 25460, 43, 52507, 40860, 68664, 25660, 31260, 75018, 74651, 47491, 57250, 12645, 56422, 57873, 38494]\n",
      "Found 2414 items\n",
      "2 datasets of sizes 1932,482\n",
      "Setting up Pipeline: get_image_by_record_id -> PILBase.create\n",
      "Setting up Pipeline: get_mask_by_record_id -> PILBase.create\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: get_image_by_record_id -> PILBase.create\n",
      "    starting from\n",
      "      8888\n",
      "    applying get_image_by_record_id gives\n",
      "      /root/data/tile-cerulean-v2-partial-with-context/tiled_images/S1A_IW_GRDH_1SDV_20210523T005625_20210523T005651_038008_047C68_FE94_vv-image_local_tile_88.tif\n",
      "    applying PILBase.create gives\n",
      "      PILImage mode=RGB size=512x512\n",
      "  Pipeline: get_mask_by_record_id -> PILBase.create\n",
      "    starting from\n",
      "      8888\n",
      "    applying get_mask_by_record_id gives\n",
      "      [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "    applying PILBase.create gives\n",
      "      PILMask mode=L size=512x512\n",
      "\n",
      "Final sample: (PILImage mode=RGB size=512x512, PILMask mode=L size=512x512)\n",
      "\n",
      "\n",
      "Collecting items from [30477, 66481, 15704, 33144, 49895, 22086, 31863, 63334, 51044, 31480, 54136, 30504, 77054, 54822, 3115, 32915, 31024, 19683, 47035, 15826, 19682, 55872, 5276, 51749, 29087, 7293, 44622, 24519, 56022, 67646, 19844, 8076, 54279, 25627, 49696, 49693, 24870, 31937, 28516, 42301, 27093, 70866, 68064, 39139, 23451, 66082, 37502, 19667, 46713, 18865, 77673, 53899, 25100, 35504, 17075, 64430, 22901, 21465, 53250, 33273, 27649, 44013, 5419, 34301, 16425, 44029, 22440, 51270, 43292, 74108, 62090, 58729, 30723, 11707, 54643, 3130, 31330, 22492, 19827, 42236, 18133, 15720, 64254, 20665, 27622, 32476, 11008, 7279, 72481, 13281, 29018, 37925, 74293, 45840, 31279, 34729, 65017, 46360, 34254, 22084, 47701, 37924, 18684, 55886, 26836, 72468, 11874, 36916, 36500, 64651, 77674, 9420, 10881, 55472, 37853, 44056, 26264, 26707, 59110, 68275, 8659, 49424, 65818, 22100, 65031, 36523, 20474, 13037, 50470, 28, 23100, 66095, 63063, 8091, 47019, 38258, 16442, 75879, 21234, 58473, 22460, 67459, 4902, 71020, 75636, 37487, 36544, 29272, 69933, 63062, 55079, 64650, 32860, 44057, 49676, 54111, 23094, 45854, 10492, 56262, 13119, 20651, 15653, 25448, 40524, 73888, 29708, 20286, 65664, 47072, 26072, 21691, 15067, 27898, 25904, 39294, 27537, 68104, 31135, 45661, 50038, 33114, 25630, 54833, 41919, 50834, 69315, 14094, 77254, 60034, 51016, 20272, 62330, 38435, 6070, 34832, 60898, 42230, 16902, 46688, 76634, 23068, 68090, 63306, 65666, 28337, 37103, 28624, 58304, 42235, 25702, 30696, 48045, 73249, 45676, 16267, 42642, 11724, 6067, 43318, 31671, 4888, 74435, 37811, 20300, 19301, 67632, 20504, 27036, 32479, 32873, 28073, 70634, 36718, 55639, 5647, 24647, 49707, 72268, 25658, 14514, 56090, 68901, 21664, 68679, 8517, 4818, 4466, 19637, 41920, 68435, 60288, 36719, 19300, 55518, 70864, 57499, 49894, 23675, 37519, 76235, 77508, 25860, 41722, 25677, 55857, 27272, 59224, 55473, 25645, 74844, 65859, 21263, 50933, 38111, 70085, 22676, 72038, 25845, 55503, 63473, 58744, 19081, 72529, 55094, 37732, 17307, 51247, 64647, 76109, 35247, 59456, 4419, 47895, 64418, 30273, 60437, 33143, 2903, 36903, 41720, 22442, 43488, 46658, 17868, 6061, 67886, 1609, 16426, 5435, 20432, 24503, 71907, 54109, 64649, 77253, 67703, 66467, 38921, 12027, 3506, 71034, 19652, 68647, 66277, 17103, 41679, 74109, 22098, 22296, 21232, 15504, 76220, 32256, 30243, 76664, 66929, 5098, 17850, 31133, 18233, 20480, 24885, 59868, 20631, 75437, 42284, 30680, 72053, 33909, 53101, 4048, 62331, 23623, 11266, 5101, 45824, 32644, 49694, 69078, 28625, 15503, 19651, 4858, 6251, 22491, 21267, 29423, 30229, 37716, 65478, 32243, 63322, 38084, 30247, 6516, 47490, 75249, 38509, 54836, 25703, 17857, 54093, 62485, 33691, 15827, 24254, 33619, 66913, 19317, 37119, 52018, 60899, 70063, 24043, 48826, 27897, 27882, 27286, 25104, 23651, 9297, 39280, 22280, 19685, 28528, 8330, 46674, 25686, 13237, 33924, 34269, 13280, 35249, 61652, 62075, 23467, 31293, 37718, 75423, 3521, 68445, 38224, 60289, 57701, 222, 54038, 59854, 15068, 3876, 71617, 44621, 30237, 56276, 70485, 4404, 25718, 54249, 70671, 40874, 74452, 70319, 30859, 2038, 52924, 31633, 32257, 56437, 34030, 49679, 17873, 72487, 33129, 49091, 23277, 57086, 18282, 34847, 2022, 69512, 32063, 68836, 37926, 73128, 64679, 19080, 65665, 58306, 37117, 62865, 68276, 66881, 64072, 37233, 20460, 11306, 28837, 29709, 61302, 45632, 20679, 8861, 32662, 51256, 12659, 25458, 76066, 27881, 40036, 22885, 55679, 9068, 38243, 63903, 40052, 47908, 21481, 16887, 29227, 25077, 17680, 12044, 34014, 17500, 54686, 69700, 70631, 47260, 28295, 22863, 68151, 35127, 22069, 54879, 67885, 25292, 40277, 55678, 32914, 64117, 34531, 14883, 36918, 64087, 30679, 629, 68867, 56625, 58036, 48536, 65704, 12317, 30221, 53251, 4287, 2080, 25473, 54234, 10101, 15737, 57503, 61336, 13890, 19462, 22475, 33690, 32685, 3098, 52668, 26672, 31134, 59452, 55826, 54657, 72485, 67502, 30695, 17837, 41082, 12871, 18886, 22900, 33132, 18682, 64405, 17853, 49089, 22833, 22477, 70847, 37530, 12834, 25613, 57085, 31631, 70332, 19684, 27051, 21822, 49710, 19464, 72276, 27035, 33272, 7280, 22873, 8892, 7861, 75047, 50052, 63032, 54135, 25422, 30874, 70686, 55532, 54832, 55515, 67702, 51222, 30520, 62095, 77520, 30230, 68049, 75865, 56465, 37733, 45848, 4721, 66914, 48210, 32932, 35723, 70084, 59699, 33507, 77255, 27634, 57856, 66945, 49632, 69045, 27054, 11252, 29242, 56851, 35219, 30222, 31659, 44028, 75635, 59713, 72501, 43020, 55694, 27097, 53918, 73235, 25293, 36486, 4314, 62693, 17872, 59468, 30249, 32493, 73655, 62301, 5110, 50510, 34046, 68884, 63294, 2255, 15721, 70318, 12831, 30493, 69513, 12021, 16268, 62710, 20632, 31237, 14528, 70724, 66260, 70655, 20087, 61434, 51045, 32845, 47715, 40293, 2639, 33657, 2681, 14527, 18849, 68434, 52034, 33117, 25719, 64666, 23866, 29942, 41041, 42845, 48924, 4288, 57487, 33228, 29710, 13222, 34031, 38906, 18119, 68136, 41122, 74306, 32286, 28294, 35873, 59700, 30234, 32874, 13908, 64241, 52312, 54701, 69529, 52032, 61304, 9450, 45868, 17834, 54672, 37702, 47476, 44019, 4285, 49451, 57097, 18669, 39430, 19862, 43507, 37248, 11254, 35874, 62678, 25306, 24031, 25661, 30231, 2653, 41708, 77509, 24030, 57715, 49422, 2710, 28610, 25708, 38239, 24869, 65266, 8904, 27316, 32506, 41932, 59853, 12025, 75265, 11253, 24703, 48224, 32496, 28668, 57096, 77519, 73672, 55500, 4300, 53412, 59404, 8285, 76306, 69545, 75050, 30233, 59840, 6502, 54120, 75217, 27509, 835, 73234, 74434, 26822, 63348, 31246, 54849, 48268, 72901, 3231, 30223, 34048, 69318, 49118, 54263, 24253, 18681, 58446, 25307, 29115, 34715, 15053, 66291, 55299, 65832, 42501, 7856, 61436, 49231, 77506, 30245, 6884, 54894, 44242, 25628, 15054, 28334, 29426, 48047, 39464, 17854, 37923, 72082, 30101, 35904, 2236, 23070, 35130, 54022, 11212, 51720, 19243, 50916, 34819, 47462, 17088, 70879, 35099, 32094, 61306, 26695, 42339, 23276, 33656, 1868, 58317, 72657, 62500, 5112, 14663, 63320, 64059, 27620, 18905, 21476, 33692, 25691, 72517, 65286, 72052, 33116, 41948, 49687, 19479, 18120, 59839, 54121, 29446, 1865, 34532, 23468, 45635, 25644, 68648, 47245, 59234, 75471, 56481, 30490, 29448, 28543, 28087, 55888, 50917, 28308, 36919, 71895, 50918, 5620, 61305, 65492, 66508, 31661, 58064, 31247, 36035, 36705, 37218, 6250, 17323, 8888, 39517, 12299, 45629, 61439, 55317, 34730, 12413, 37687, 30849, 39518, 72530, 66931, 70880, 13514, 40649, 50641, 26835, 28823, 3466, 74305, 29640, 75237, 22085, 67520, 69062, 34831, 69317, 645, 53402, 33130, 43019, 9821, 3859, 17851, 35246, 33641, 64431, 71908, 21677, 49216, 33623, 19272, 45849, 16443, 57504, 33689, 19828, 66898, 31892, 29086, 49646, 11230, 26420, 22915, 47731, 54265, 74321, 52519, 15530, 29, 18487, 30861, 9668, 36687, 33639, 7292, 26431, 7096, 66894, 29652, 42317, 26071, 45634, 62499, 25462, 56464, 35234, 27895, 23484, 15670, 27288, 16696, 58292, 16423, 64073, 50884, 49896, 26723, 67899, 71281, 56639, 27477, 77687, 36887, 21088, 37220, 5261, 71517, 36365, 7862, 58274, 66480, 57700, 68319, 1284, 31235, 25659, 10897, 28100, 15055, 41287, 34013, 61672, 23259, 67487, 50637, 19512, 55681, 63049, 37118, 38907, 70702, 35887, 70333, 54043, 18693, 34268, 73250, 30248, 31621, 426, 36889, 34253, 47504, 20489, 19699, 26131, 57823, 7871, 48825, 57500, 19843, 40881, 20447, 23066, 4272, 31465, 61695, 39110, 34514, 25423, 27690, 65845, 34685, 38631, 50657, 53076, 19671, 32242, 49633, 68422, 62107, 9835, 26450, 14632, 37204, 4302, 52233, 48535, 55680, 51245, 16285, 20503, 70022, 51257, 13515, 28838, 76110, 12833, 4844, 8302, 30863, 626, 23619, 20462, 54908, 54092, 28824, 8300, 73142, 74277, 30481, 28306, 74433, 72515, 74920, 30251, 61454, 34829, 13839, 43903, 31039, 70896, 64253, 62866, 43904, 68118, 14122, 3450, 31044, 62094, 66947, 37704, 58260, 72486, 24632, 19066, 27101, 65690, 29916, 68837, 30071, 51249, 67872, 57872, 76435, 35918, 43834, 50100, 5605, 72916, 8301, 27083, 34285, 76094, 31645, 32021, 23638, 32900, 62089, 37954, 53901, 71603, 36048, 25463, 36930, 27047, 33473, 75523, 2682, 37909, 77069, 75053, 63336, 50053, 25299, 19670, 33908, 73887, 10299, 20446, 47231, 71503, 72484, 21466, 58288, 7474, 60623, 19045, 36034, 34529, 72900, 37130, 65030, 54656, 63285, 20278, 56108, 13823, 51261, 13530, 27691, 6699, 37533, 52940, 67818, 27130, 35233, 13118, 58303, 25687, 31662, 56924, 52505, 5888, 19463, 3114, 70516, 28667, 66110, 29493, 53898, 46034, 3451, 41949, 59123, 6068, 28323, 53289, 66523, 73220, 20488, 3683, 75451, 70862, 69528, 75048, 17483, 857, 63708, 23690, 61273, 19316, 23052, 37673, 31262, 12029, 75483, 66915, 19838, 12818, 73624, 20459, 22691, 31649, 18489, 32887, 43317, 62694, 52019, 16117, 11890, 1018, 55695, 54424, 37102, 22493, 8221, 45314, 42639, 36515, 19257, 25447, 72915, 49450, 72067, 23882, 65706, 30845, 43463, 51235, 12430, 9291, 19686, 41678, 39094, 73205, 65253, 74921, 76649, 23275, 60427, 10055, 71268, 48819, 7625, 26465, 25918, 34541, 50011, 65032, 26706, 20494, 14096, 24633, 67660, 3100, 11444, 60913, 2473, 37503, 61657, 48266, 67675, 23466, 31294, 14123, 61909, 14136, 66081, 22913, 35888, 75051, 77532, 76234, 26300, 36933, 29424, 42220, 49425, 32717, 27663, 75264, 51734, 31051, 53917, 61437, 77510, 95, 54075, 63275, 18132, 7490, 14220, 56039, 9424, 8878, 67898, 37250, 8531, 46345, 62345, 72532, 24851, 71035, 55858, 20895, 51438, 20647, 3841, 72277, 30873, 25676, 48733, 7857, 37088, 33476, 76844, 48046, 77516, 51835, 22266, 77521, 37473, 50039, 20431, 35141, 30232, 74768, 55076, 41917, 24022, 76633, 68852, 22426, 64664, 11231, 72670, 33292, 62879, 31648, 64869, 41694, 66900, 23494, 12856, 56480, 53664, 6265, 41055, 24240, 30235, 11286, 31233, 54027, 62515, 25690, 8265, 19044, 72283, 14633, 1893, 8514, 20650, 23258, 25629, 25671, 26057, 70662, 34818, 68833, 53110, 35889, 53236, 16697, 25674, 2239, 34857, 54264, 8496, 61318, 5087, 69027, 60428, 27538, 26464, 24853, 75637, 3096, 25091, 2457, 33427, 1892, 30087, 68853, 23051, 75419, 31466, 54441, 25662, 66713, 27063, 68031, 23867, 9437, 20085, 20263, 4859, 32646, 3097, 67661, 55517, 61334, 74875, 57839, 70470, 68900, 65817, 5246, 24867, 25487, 64654, 50899, 18729, 58291, 15039, 75252, 3520, 64868, 55264, 48894, 35700, 29929, 21101, 77518, 76648, 49273, 33923, 49287, 40648, 73431, 54834, 27675, 27496, 72494, 38935, 53665, 23109, 75037, 34060, 37234, 6501, 66262, 75469, 30224, 69316, 42103, 39336, 61321, 25050, 13235, 36931, 32507, 68290, 65258, 35724, 31635, 43634, 12022, 24518, 50041, 23883, 2667, 14110, 3860, 73090, 2917, 35117, 34744, 39637, 23865, 71634, 43635, 16136, 23083, 61658, 21118, 72516, 77068, 36904, 59453, 37719, 16886, 44240, 36900, 47246, 74291, 57482, 51432, 71253, 39351, 10315, 30296, 51693, 36733, 69903, 49066, 73107, 26279, 11443, 40308, 33915, 76237, 62880, 64443, 67458, 43622, 52670, 70007, 27510, 44510, 2441, 54059, 65506, 22108, 27525, 25834, 22094, 69028, 34062, 17624, 49245, 63681, 76650, 30117, 62713, 34724, 35090, 75049, 21491, 33910, 75453, 4017, 9038, 31050, 37689, 70863, 70291, 66274, 20277, 32062, 30837, 54110, 76864, 60032, 22476, 31023, 56853, 51679, 4690, 35933, 48252, 71618, 17867, 65692, 7282, 51431, 28839, 34068, 73115, 33229, 39079, 32494, 70672, 37952, 30072, 60912, 17849, 75435, 22878, 50639, 56056, 20069, 21132, 57444, 76863, 23067, 36702, 19694, 45862, 77507, 25706, 73106, 21493, 27331, 29440, 33673, 40068, 11229, 37236, 27078, 65639, 33640, 42846, 20287, 31646, 5102, 29454, 411, 26246, 39322, 20476, 32109, 8236, 13531, 24252, 21479, 64682, 52672, 60443, 77686, 67882, 11250, 12034, 3277, 29928, 17060, 19698, 21848, 72083, 13840, 57857, 62711, 22461, 69917, 56923, 62698, 18883, 29681, 71049, 38083, 58035, 73135, 13873, 35221, 3113, 26432, 37133, 37101, 34061, 28836, 9296, 66522, 22914, 25120, 15722, 47909, 13824, 56852, 43304, 55887, 44, 38450, 48923, 33257, 63286, 18067, 3278, 38451, 25646, 36530, 11268, 22427, 61108, 74307, 69686, 57498, 37074, 29638, 76472, 16284, 61667, 69530, 38920, 29915, 44606, 72097, 67617, 49659, 72493, 49423, 31660, 63439, 38082, 49067, 52279, 76877, 32480, 55650, 8532, 68649, 55873, 55902, 76061, 72686, 59106, 70501, 17643, 3436, 12297, 46729, 37104, 12318, 5099, 68152, 53090, 9455, 18502, 43464, 76841, 61322, 16871, 15305, 71633, 36022, 1880, 21704, 38645, 77672, 77517, 6046, 64665, 29085, 67519, 4299, 33291, 30262, 71906, 25611, 23639, 24504, 61435, 43863, 22847, 29100, 30518, 60285, 12872, 55282, 46346, 36917, 22888, 17852, 28530, 75251, 9441, 29467, 31647, 32920, 76108, 61922, 22494, 20314, 67473, 25832, 25064, 25105, 55874, 28542, 27883, 9653, 31673, 7111, 4315, 4248, 57483, 25876, 42643, 8515, 37730, 11425, 61319, 12819, 5292, 8317, 49651, 76857, 4722, 65272, 205, 70093, 32913, 67865, 22052, 36019, 66883, 20273, 20506, 1266, 23665, 33492, 55697, 64058, 22899, 26430, 10255, 4286, 17306, 55871, 5233, 62302, 17087, 2066, 54818, 2638, 37922, 44041, 4062, 29434, 54231, 27095, 30287, 59288, 16424, 67291, 32919, 57854, 32859, 8889, 59712, 69514, 68304, 22109, 37222, 43021, 18488, 64417, 64057, 58445, 29226, 23084, 21233, 21821, 45892, 47716, 22281, 32933, 48225, 427, 18730, 47232, 63290, 26278, 55903, 20461, 27491, 44867, 57855, 8315, 8860, 35715, 29466, 65252, 31038, 64669, 70630, 43673, 75468, 21495, 38499, 63695, 70731, 31658, 34284, 48522, 73120, 44524, 67618, 25098, 67521, 33914, 56626, 38484, 52669, 37087, 46911, 64102, 18694, 33260, 1004, 68421, 66916, 32481, 25709, 12033, 63065, 3101, 64648, 73898, 24036, 58521, 35489, 61320, 32491, 37205, 29019, 64667, 31030, 39308, 38630, 14500, 38452, 37953, 21650, 71602, 17067, 31053, 47204, 53102, 58050, 42641, 77039, 31344, 34300, 24894, 15529, 12855, 54893, 27896, 30862, 27039, 11424, 14234, 45839, 11271, 70716, 43291, 44035, 39901, 4259, 72291, 4479, 73447, 63694, 42089, 54700, 30489, 73121, 31674, 11248, 27287, 56107, 11251, 7877, 38493, 18668, 45620, 43848, 24225, 29004, 49708, 35114, 21496, 30100, 15546, 38069, 12298, 31686, 47717, 14095, 45690, 48267, 15846, 47016, 4819, 8266, 16901, 24037, 45675, 19861, 75648, 49690, 50248, 28307, 18866, 73092, 30102, 26117, 3874, 65691, 35132, 75634, 60033, 17061, 41054, 64652, 68680, 53067, 72669, 24883, 23868, 43623, 22458, 63680, 29271, 29435, 24042, 10493, 39279, 17059, 19668, 30286, 18692, 55091, 59257, 69044, 46436, 67472, 45516, 32901, 13220, 17871, 14109, 69010, 2260, 36905, 43877, 72274, 58856, 25820, 43890, 10616, 31891, 65259, 4467, 72502, 54862, 63047, 48833, 66882, 34862, 62514, 19064, 22425, 31037, 33275, 63456, 42251, 35260, 25063, 51018, 31064, 7110, 6700, 56423, 61109, 34861, 11272, 24884, 64255, 3293, 28101, 67111, 54127, 28529, 31304, 51735, 23069, 72544, 2902, 33491, 68050, 20086, 34528, 66884, 76236, 19824, 19271, 73656, 19825, 56277, 850, 27635, 33674, 75434, 41918, 73104, 6900, 34686, 69011, 16444, 33457, 64444, 29253, 37132, 33675, 30521, 77531, 2680, 57838, 12036, 21494, 68105, 61123, 69684, 17679, 12431, 29653, 42300, 65029, 42266, 2254, 29930, 35113, 36875, 52491, 56075, 66820, 66288, 69061, 74276, 67488, 35708, 70895, 44852, 32222, 73671, 15036, 22459, 22675, 26837, 32918, 22083, 59663, 67290, 34517, 68274, 30256, 206, 630, 30492, 64419, 75484, 69916, 49688, 39636, 31063, 25614, 27676, 60286, 8890, 3465, 3246, 69902, 45689, 37100, 11267, 25322, 71295, 30503, 6865, 21705, 26287, 7296, 39109, 69918, 29422, 30057, 45615, 25610, 32699, 1819, 29238, 68137, 29099, 34527, 32223, 45904, 62697, 76253, 70078, 36704, 10256, 32899, 75524, 4047, 63905, 46923, 14515, 45040, 34848, 14648, 26260, 29439, 53088, 5874, 49217, 44657, 25626, 53897, 27476, 33141, 59469, 69497, 31105, 31622, 65678, 72658, 1867, 37073, 75052, 4478, 5111, 29237, 21820, 35235, 26433, 50640, 18715, 70069, 22690, 75880, 20633, 66289, 29694, 14108, 21849, 58728, 34530, 60284, 11287, 65505, 76065, 23452, 27062, 70717, 31623, 70865, 76233, 46897, 2665, 19418, 63048, 50883, 59686, 21248, 24914, 17431, 68678, 49637, 50496, 50471, 59125, 42640, 12832, 67292, 20068, 51439, 8511, 76062, 43022, 31251, 69531, 48269, 44624, 9438, 49318, 17838, 4334, 48732, 66712, 11210, 72275, 72463, 38919, 76433, 47730, 11249, 42252, 67503, 59287, 8831, 48910, 19858, 43278, 18116, 17499, 36703, 39915, 22428, 75036, 45838, 50511, 47687, 4061, 55856, 45039, 10102, 50012, 4707, 11009, 9426, 30058, 68444, 44509, 68869, 71267, 76083, 16286, 21074, 54837, 52033, 47218, 29252, 40665, 26830, 23663, 75438, 2666, 73218, 39916, 33475, 37219, 21817, 20102, 58520, 37131, 51433, 51692, 3858, 27673, 66851, 56479, 76290, 8495, 76249, 67518, 14121, 19478, 28072, 76843, 37517, 44039, 7291, 50901, 11709, 28873, 28874, 22441, 52939, 5100, 64268, 59256, 47020, 25672, 17888, 2235, 10896, 46438, 45869, 31278, 15636, 7094, 28822, 40070, 71296, 31052, 39870, 16118, 26831, 21699, 25460, 43, 52507, 40860, 68664, 25660, 31260, 75018, 74651, 47491, 57250, 12645, 56422, 57873, 38494]\n",
      "Found 2414 items\n",
      "2 datasets of sizes 1932,482\n",
      "Setting up Pipeline: get_image_by_record_id -> PILBase.create\n",
      "Setting up Pipeline: get_mask_by_record_id -> PILBase.create\n",
      "Setting up after_item: Pipeline: AddMaskCodes -> ToTensor\n",
      "Setting up before_batch: Pipeline: \n",
      "Setting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -> Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: AddMaskCodes -> ToTensor\n",
      "    starting from\n",
      "      (PILImage mode=RGB size=512x512, PILMask mode=L size=512x512)\n",
      "    applying AddMaskCodes gives\n",
      "      (PILImage mode=RGB size=512x512, PILMask mode=L size=512x512)\n",
      "    applying ToTensor gives\n",
      "      (TensorImage of size 3x512x512, TensorMask of size 512x512)\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "No before_batch transform to apply\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -> Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}\n",
      "    starting from\n",
      "      (TensorImage of size 4x3x512x512, TensorMask of size 4x512x512)\n",
      "    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n",
      "      (TensorImage of size 4x3x512x512, TensorMask of size 4x512x512)\n",
      "    applying Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} gives\n",
      "      (TensorImage of size 4x3x512x512, TensorMask of size 4x512x512)\n",
      "    applying Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False} gives\n",
      "      (TensorImage of size 4x3x512x512, TensorMask of size 4x512x512)\n"
     ]
    }
   ],
   "source": [
    "coco_seg_dblock.summary(record_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai2 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
    "experiment_dir =  Path(f'{mount_path}/experiments/cv2/'+timestampStr+'_fastai_unet/')\n",
    "experiment_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 50\n",
    "archs = {18: resnet18, 34: resnet34, 50: resnet50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80afa3a18a164110bc06fd7950f4ef82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/386 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 728.00 MiB (GPU 0; 14.76 GiB total capacity; 11.34 GiB already allocated; 209.75 MiB free; 13.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m learn \u001b[38;5;241m=\u001b[39m unet_learner(dls, archs[arch], metrics\u001b[38;5;241m=\u001b[39m[Dice()], model_dir\u001b[38;5;241m=\u001b[39mexperiment_dir, n_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m, cbs\u001b[38;5;241m=\u001b[39m[MixedPrecision]) \u001b[38;5;66;03m# cbs=[MixedPrecision]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m cbs \u001b[38;5;241m=\u001b[39m [TensorBoardCallback(projector\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, trace_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m      7\u001b[0m learn\u001b[38;5;241m.\u001b[39mfine_tune(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2e-4\u001b[39m, cbs\u001b[38;5;241m=\u001b[39mcbs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/callback/schedule.py:289\u001b[0m, in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, num_it, stop_div, show_plot, suggest_funcs)\u001b[0m\n\u001b[1;32m    287\u001b[0m n_epoch \u001b[38;5;241m=\u001b[39m num_it\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    288\u001b[0m cb\u001b[38;5;241m=\u001b[39mLRFinder(start_lr\u001b[38;5;241m=\u001b[39mstart_lr, end_lr\u001b[38;5;241m=\u001b[39mend_lr, num_it\u001b[38;5;241m=\u001b[39mnum_it, stop_div\u001b[38;5;241m=\u001b[39mstop_div)\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_logging(): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suggest_funcs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     lrs, losses \u001b[38;5;241m=\u001b[39m tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorder\u001b[38;5;241m.\u001b[39mlrs[num_it\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]), tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorder\u001b[38;5;241m.\u001b[39mlosses[num_it\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:221\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:206\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:198\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:169\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:194\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    192\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb):\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/layers.py:407\u001b[0m, in \u001b[0;36mSequentialEx.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    406\u001b[0m     res\u001b[38;5;241m.\u001b[39morig \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 407\u001b[0m     nres \u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# We have to remove res.orig to avoid hanging refs and therefore memory leaks\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     res\u001b[38;5;241m.\u001b[39morig, nres\u001b[38;5;241m.\u001b[39morig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/layers.py:489\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[0;32m--> 489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midpath(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"relu(input, inplace=False) -> Tensor\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \n\u001b[1;32m   1291\u001b[0m \u001b[38;5;124;03mApplies the rectified linear unit function element-wise. See\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;124;03m:class:`~torch.nn.ReLU` for more details.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   1297\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/overrides.py:1355\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1350\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in PyTorch 1.11, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1351\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/torch_core.py:341\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opt, func): convert,types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m),(torch\u001b[38;5;241m.\u001b[39mTensor,)\n\u001b[0;32m--> 341\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert: res \u001b[38;5;241m=\u001b[39m convert(res)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, TensorBase): res\u001b[38;5;241m.\u001b[39mset_meta(\u001b[38;5;28mself\u001b[39m, as_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/_tensor.py:1051\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1051\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1053\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.9/site-packages/torch/nn/functional.py:1299\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1299\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 0; 14.76 GiB total capacity; 11.34 GiB already allocated; 209.75 MiB free; 13.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "learn = unet_learner(dls, archs[arch], metrics=[Dice()], model_dir=experiment_dir, n_out = 7, cbs=[MixedPrecision]) # cbs=[MixedPrecision]\n",
    "\n",
    "lr = learn.lr_find()\n",
    "\n",
    "cbs = [TensorBoardCallback(projector=False, trace_model=False)]\n",
    "\n",
    "learn.fine_tune(2, 2e-4, cbs=cbs)#, cbs=SaveModelCallback(monitor='dice'))w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = f'{arch}_{size}_{round(validation[1],3)}.pkl'\n",
    "learn.export(f'{experiment_dir}/{savename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJQ3VVuD9rx2"
   },
   "outputs": [],
   "source": [
    "learn.show_results(max_n=4, figsize=(20,20), vmin=0, vmax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path for tensorboard logs is `./runs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls './runs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy logs to appropriate exeriments folder in the mounted GCS volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R './runs/' {modelpath}'/tensorboard/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following from anywhere with gcs authenticated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=\"./runs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference and Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = torch.load(\"/root/data/experiments/cv2/10_May_2022_18_02_59_fastai_unet/18_64_0.493.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm_type_tfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Prediction on `item`, fully decoded, loss function decoded and probabilities\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm_type_tfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm_type_tfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrm_type_tfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_decoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_inp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_targ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetuplify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_targ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mwith_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/fastai2/lib/python3.9/site-packages/fastai/learner.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.predict??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skimage.io as skio\n",
    "val_record_ids = record_collection_to_record_ids(valid_records)\n",
    "pred_arrs = []\n",
    "with learner.no_logging():\n",
    "    for i in val_record_ids:\n",
    "        p = get_image_path(valid_records,i)\n",
    "        arr = skio.imread(p)\n",
    "        pred_arr = learner.predict(arr)\n",
    "        pred_arrs.append(pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this results in vm dying, not just kernel crash\n",
    "# coco_seg_dblock = DataBlock(\n",
    "#     blocks=(ImageBlock, MaskBlock(codes=class_ints)),\n",
    "#     get_x=get_image_by_record_id,\n",
    "#     get_y=get_mask_by_record_id,\n",
    "#     n_inp=1,\n",
    "# )\n",
    "\n",
    "# dls = coco_seg_dblock.dataloaders(source=record_ids, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = learner.get_preds(dl=dls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.get_preds??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label,prediction_arr, activations = pred_arrs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(target_label.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(base_img.cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(base_img.cpu().detach().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(base_img.cpu().detach().numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(base_img.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([      60.73,       190.3,      4.3598]) # means\n",
    "array([     16.099,      17.846,       9.603]) # stats"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fastai2_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
