{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bENUAXPgDEkl"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "from fastai.callback.fp16 import *\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "import json\n",
        "import wandb\n",
        "from fastai.callback.wandb import WandbCallback\n",
        "import random\n",
        "from pycocotools import mask\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grab_tile_files_from_partition(data, percent_empty):\n",
        "    masked_image_ids = list(set([ann['image_id'] for ann in data['annotations']]))\n",
        "    masked_image_files = [im['file_name'] for im in data['images'] if im['id'] in masked_image_ids]\n",
        "    unmasked_image_files = [im['file_name'] for im in data['images'] if not (im['id'] in masked_image_ids)]\n",
        "\n",
        "    masked_num = len(masked_image_files)\n",
        "    # unmasked_num = len(unmasked_image_files)\n",
        "\n",
        "    num_empty = int(percent_empty*masked_num / (1-percent_empty))\n",
        "\n",
        "    #num_empty = int(percent_empty*unmasked_num)\n",
        "\n",
        "    print(\"Returning\", masked_num, \"Masked Tile Files and\", num_empty, \"Unmasked Tile Files\")\n",
        "\n",
        "    return [masked_image_files , random.sample(unmasked_image_files, num_empty)]\n",
        "\n",
        "def get_masks_labels_for_file(file, ds):\n",
        "  \n",
        "  id_map = {}\n",
        "  for im in ds['images']:\n",
        "    id_map[im['file_name']] = im['id']\n",
        "  labels = []\n",
        "  masks = []\n",
        "  for ann in ds['annotations']:\n",
        "    if ann['image_id'] == id_map[file]:\n",
        "      print(ann.keys())\n",
        "      labels.append(ann['category_id'])\n",
        "      masks.append(ann['segmentation'])\n",
        "  return masks, labels\n",
        "\n",
        "def grab_and_download_tile_and_mask(tile_location,save_location, file, ds):\n",
        "  if not os.path.exists(save_location):\n",
        "      os.makedirs(save_location)\n",
        "  masks,labels = get_masks_labels_for_file(file, ds)\n",
        "  mask_canvas = np.zeros(shape= (1024,1024))\n",
        "  for i,m in enumerate(masks):\n",
        "    label = labels[i]\n",
        "    rle = mask.frPyObjects(m, 1024, 1024)\n",
        "    mask_canvas+=mask.decode(rle)*label\n",
        "  \n",
        "  with rasterio.open(tile_location+file) as src:\n",
        "    # Read the first band\n",
        "    band1 = src.read(1)\n",
        "\n",
        "  # Normalize the band data to the range 0-255\n",
        "  band1_normalized = (band1).astype(np.uint8)\n",
        "\n",
        "  # return band1_normalized, mask_canvas\n",
        "  # Create an Image object from the normalized data\n",
        "  vv = Image.fromarray(band1_normalized)\n",
        "  mask_im = Image.fromarray(mask_canvas.astype(np.uint8))\n",
        "\n",
        "  vv.save(save_location + file[:-4] + \".png\")\n",
        "  mask_im.save(save_location + file[:-4] + \"_mask.png\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_path = \"/root/partitions/train_tiles_context_1024/\"\n",
        "with open(split_path+'instances_TiledCeruleanDatasetV2.json') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "masked_files, unmasked_files = grab_tile_files_from_partition(data, .1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = get_masks_labels_for_file(masked_files[0], data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "masked_files[0][:-4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grab_and_download_tile_and_mask(\"/root/partitions/train_tiles_context_1024/tiled_images/\",\n",
        "                                \"/root/partitions/tiles_context_1024_png/train/\",\n",
        "                                unmasked_files[0], \n",
        "                                data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(\"/root\")\n",
        "maskerino = Image.open(\"partitions/tiles_context_1024_png/train/S1A_IW_GRDH_1SDV_20210323T142456_20210323T142521_037127_045F0F_F358_vv-image_local_tile_9_mask.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "maskerino = Image.fromarray(np.array(maskerino)*255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "maskerino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_path = \"/root/partitions/train_tiles_context_1024/\"\n",
        "with open(split_path+'/instances_TiledCeruleanDatasetV2.json') as f:\n",
        "    data = json.load(f)\n",
        "masked_files, unmasked_files = grab_tile_files_from_partition(data, .1)\n",
        "\n",
        "for masked_file in masked_files:\n",
        "    grab_and_download_tile_and_mask(\"/root/partitions/train_tiles_context_1024/tiled_images/\",\n",
        "                                \"/root/partitions/tiles_context_1024_png/train/\",\n",
        "                                masked_file, \n",
        "                                data)\n",
        "for unmasked_file in unmasked_files:\n",
        "    grab_and_download_tile_and_mask(\"/root/partitions/train_tiles_context_1024/tiled_images/\",\n",
        "                            \"/root/partitions/tiles_context_1024_png/train/\",\n",
        "                            unmasked_file, \n",
        "                            data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.listdir(\"/root/partitions/val_tiles_context_1024\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_path = \"/root/partitions/val_tiles_context_1024/\\r\"\n",
        "with open(split_path+'/instances_TiledCeruleanDatasetV2.json') as f:\n",
        "    data = json.load(f)\n",
        "masked_files, unmasked_files = grab_tile_files_from_partition(data, .1)\n",
        "\n",
        "for masked_file in masked_files:\n",
        "    grab_and_download_tile_and_mask(\"/root/partitions/val_tiles_context_1024/\\r/tiled_images/\",\n",
        "                                \"/root/partitions/tiles_context_1024_png/valid/\",\n",
        "                                masked_file, \n",
        "                                data)\n",
        "\n",
        "# for unmasked_file in unmasked_files:\n",
        "#     grab_and_download_tile_and_mask(\"/root/partitions/val_tiles_context_1024/tiled_images/\",\n",
        "#                             \"/root/partitions/tiles_context_1024_png/train/\",\n",
        "#                             unmasked_file, \n",
        "#                             data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learner_config = json.load(open(\"/root/work/starter_learner_config.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model / Learner params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freeze_epochs =  learner_config[\"freeze_epochs\"] #\n",
        "unfreeze_epochs = learner_config[\"unfreeze_epochs\"] #\n",
        "loss_func_name = learner_config[\"loss_func\"] #\n",
        "w_d = learner_config[\"w_d\"] #\n",
        "backbone = learner_config[\"backbone\"] #\n",
        "final_resize_resolution = learner_config[\"final_resize_resolution\"] #\n",
        "num_of_workers = learner_config[\"num_of_workers\"] #\n",
        "progressive_resizing = learner_config[\"progressive_resizing\"] #\n",
        "batch_size = learner_config[\"batch_size\"] #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Params (Not Relevent to this script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = learner_config[\"classes\"]\n",
        "area_thresh = learner_config[\"area_thresh\"]\n",
        "negative_sample_count_train = learner_config[\"negative_sample_count_train\"]\n",
        "aux_channels = learner_config[\"aux_channels\"]\n",
        "scale_limit = learner_config[\"scale_limit\"]\n",
        "rotate_limit = learner_config[\"rotate_limit\"]\n",
        "r_g_b_shift_limit = learner_config[\"r/g/b_shift_limit\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVov7m_DP2k4"
      },
      "outputs": [],
      "source": [
        "class NormalizeAndClamp(Transform):\n",
        "    def __init__(self, mean, std, min_val=-3, max_val=3):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.min_val = min_val\n",
        "        self.max_val = max_val\n",
        "\n",
        "    def encodes(self, x):\n",
        "        x = (x - self.mean) / self.std\n",
        "        x = x.clamp(min=self.min_val, max=self.max_val)\n",
        "        return x\n",
        "\n",
        "def get_image_files_no_masks(path, pct_negative=0.0):\n",
        "    \"\"\"Collect image files that don't end with '_mask.png'.\"\"\"\n",
        "    empty_masks = []\n",
        "    non_empty_masks = []\n",
        "\n",
        "    all_files = get_image_files(path)\n",
        "    masks = [f for f in all_files if f.name.endswith(\"_mask.png\")]\n",
        "\n",
        "    for m in tqdm(masks):\n",
        "        with Image.open(m) as img:\n",
        "            if not img.getbbox():\n",
        "                # This is a quick check to see if the image is completely black or transparent.\n",
        "                empty_masks.append(m)\n",
        "            else:\n",
        "                non_empty_masks.append(m)\n",
        "    empty_count = int(pct_negative * len(empty_masks))\n",
        "    print(f\"Training on {len(non_empty_masks)} non-empty masks, and {empty_count} empty masks\")\n",
        "    fileset = non_empty_masks + empty_masks[:empty_count]\n",
        "    return [str(f).replace('_mask', '') for f in fileset]\n",
        "\n",
        "def get_y_seg(x_path):\n",
        "    return x_path.replace('.png', '_mask.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7GeF4bMPjy-"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Fastai DataLoaders for grayscale images\n",
        "path_seg = \"/root/partitions/tiles_context_1024_png\"  # Dataset's path\n",
        "ParentSplitter_seg = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
        "SAR_stats = [0.2087162, 0.13736105] # Calculated from the entire training dataset\n",
        "\n",
        "codes = ['background', 'infrastructure', 'natural', 'vessel_coincident', 'vessel_recent', 'vessel_old', 'ambiguous']\n",
        "\n",
        "cbs_seg = [TerminateOnNaNCallback(), GradientAccumulation(8), GradientClip(), SaveModelCallback(), ShowGraphCallback()]\n",
        "#  ShortEpochCallback(pct=0.1, short_valid=False),\n",
        "# EarlyStoppingCallback(min_delta=.001, patience=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "oxMZZwiGH-tg",
        "outputId": "c4a52633-e8a1-438e-b9b7-d460a8b91512"
      },
      "outputs": [],
      "source": [
        "def get_dataloader_seg(SIZE, resize, batch_size, num_of_workers):\n",
        "\n",
        "    # Define a DataBlock and dataloaders\n",
        "    datablock_seg = DataBlock(\n",
        "        blocks= [ImageBlock(cls=PILImageBW), MaskBlock(codes = codes)], # Use ImageBlock twice for autoencoder, or (ImageBlock, Maskblock) segmentation etc.\n",
        "        get_items=get_image_files_no_masks,\n",
        "        splitter=ParentSplitter_seg,  # Split based on folder names\n",
        "        get_y=get_y_seg,  # For autoencoders, where input x is the target y. Adjust if necessary.\n",
        "        batch_tfms = aug_transforms(do_flip=True, flip_vert=True, pad_mode=PadMode.Zeros, size=(SIZE, SIZE)),\n",
        "        item_tfms = Resize(resize)\n",
        "    )\n",
        "\n",
        "    dls_seg = datablock_seg.dataloaders(path_seg, path=path_seg, bs=batch_size, num_workers=num_of_workers).to(device)\n",
        "    dls_seg.after_item = Pipeline([ToTensor(), IntToFloatTensor()])\n",
        "    dls_seg.after_batch = Pipeline([NormalizeAndClamp(*SAR_stats)])\n",
        "    return dls_seg\n",
        "\n",
        "#learn_baseline=unet_learner(dls=dls_seg, arch=resnet101, loss_func=loss_func, cbs=cbs_seg, n_in=1, lr=1e-3, wd=1e-3) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_configured_learner(dls_seg, backbone=backbone, loss_func_name=loss_func_name, cbs_seg=cbs_seg, lr=1e-3, wd=w_d):\n",
        "    loss_func = globals()[loss_func_name](ignore_index=6, axis=1)\n",
        "    if \"convnext\" in backbone:\n",
        "        model_func = globals()[backbone]\n",
        "        body = create_body(model_func(), 1, pretrained=True)\n",
        "        unet = DynamicUnet(body[0], n_out=7, img_size = (128,128))\n",
        "    return Learner(dls_seg, unet, loss_func=loss_func, cbs=cbs_seg, lr=lr, wd=wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip4Y2_7BhrUg"
      },
      "outputs": [],
      "source": [
        "def train_loop(freeze_epochs,unfreeze_epochs, progressive_resizing=progressive_resizing):\n",
        "    progressive_dls = []\n",
        "    for resize in progressive_resizing:\n",
        "        progressive_dls.append(get_dataloader_seg(final_resize_resolution, resize, batch_size, num_of_workers))\n",
        "    #freeze training\n",
        "    learner = build_configured_learner(progressive_dls[0])\n",
        "\n",
        "    for i,dls in enumerate(progressive_dls):\n",
        "        learner.dls = dls\n",
        "        fr_ep = int(freeze_epochs/len(progressive_dls))\n",
        "\n",
        "        print(\"Freezing encoder and training for\", fr_ep, \"epochs at \", progressive_resizing[i], \"input size..,\")\n",
        "        for param in learner.model[0].parameters():\n",
        "            param.requires_grad = False\n",
        "        learner.fit_one_cycle(fr_ep)\n",
        "\n",
        "    for i,dls in enumerate(progressive_dls):\n",
        "        learner.dls = dls\n",
        "        un_ep = int(unfreeze_epochs/len(progressive_dls))\n",
        "        print(\"Unfreezing encoder and training for\", un_ep, \"epochs at \", progressive_resizing[i], \"input size...\")\n",
        "        for param in learner.model[0].parameters():\n",
        "            param.requires_grad = True\n",
        "        learner.fit_one_cycle(un_ep)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loop(freeze_epochs=2, unfreeze_epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dls = get_dataloader_seg(final_resize_resolution, 512, batch_size, num_of_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dls.show_batch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
